{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87de52b8-adc3-4f35-a740-a7cb9b8e0ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME = /home/jovyan/shares/SR008.fs2/iana_kulichenko/cuda-12.4\n",
      "PATH = /home/jovyan/shares/SR008.fs2/iana_kulichenko/cuda-12.4/bin:/home/jovyan/.mlspace/envs/deepseek_iana/bin:/home/jovyan/shares/SR008.fs2/iana_kulichenko/cuda-12.4/bin:/home/jovyan/.mlspace/envs/deepseek_iana/bin:/home/jovyan/.mlspace/envs/deepseek_iana/bin:/home/user/conda/condabin:/home/user/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/user/conda/bin\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Tue_Feb_27_16:19:38_PST_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.99\n",
      "Build cuda_12.4.r12.4/compiler.33961263_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] = \"/home/jovyan/.mlspace/envs/deepseek_iana/bin:\" + os.environ[\"PATH\"]\n",
    "os.environ[\"CUDA_HOME\"] = \"/home/jovyan/shares/SR008.fs2/iana_kulichenko/cuda-12.4\"\n",
    "os.environ[\"PATH\"] = os.path.join(os.environ[\"CUDA_HOME\"], \"bin\") + \":\" + os.environ[\"PATH\"]\n",
    "print(\"CUDA_HOME =\", os.environ[\"CUDA_HOME\"])\n",
    "print(\"PATH =\", os.environ[\"PATH\"])\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd383464-5415-42d1-94fc-a77e7caec674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc09046-f3e1-4b26-99f3-2c04709af682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-23 00:03:37,112] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-23 00:03:37,114] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-23 00:03:37,119] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-23 00:03:37,122] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-23 00:03:37,124] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-23 00:03:37,125] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-23 00:03:37,127] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-02-23 00:03:37,131] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaaaaaaaaaa-nana\u001b[0m (\u001b[33miaaaaaaaaaa-nana-aon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaaaaaaaaaa-nana\u001b[0m (\u001b[33miaaaaaaaaaa-nana-aon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaaaaaaaaaa-nana\u001b[0m (\u001b[33miaaaaaaaaaa-nana-aon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaaaaaaaaaa-nana\u001b[0m (\u001b[33miaaaaaaaaaa-nana-aon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaaaaaaaaaa-nana\u001b[0m (\u001b[33miaaaaaaaaaa-nana-aon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaaaaaaaaaa-nana\u001b[0m (\u001b[33miaaaaaaaaaa-nana-aon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaaaaaaaaaa-nana\u001b[0m (\u001b[33miaaaaaaaaaa-nana-aon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miaaaaaaaaaa-nana\u001b[0m (\u001b[33miaaaaaaaaaa-nana-aon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/wandb/run-20250223_000345-yguzo573\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2D_Layout_20250223_000345\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/yguzo573\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/wandb/run-20250223_000345-u2gq02r6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2D_Layout_20250223_000345\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/u2gq02r6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/wandb/run-20250223_000345-vm2otlpu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2D_Layout_20250223_000345\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/vm2otlpu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/wandb/run-20250223_000345-5g6fg6sp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2D_Layout_20250223_000345\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/5g6fg6sp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/wandb/run-20250223_000345-abda5k44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2D_Layout_20250223_000345\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/abda5k44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/wandb/run-20250223_000345-el55jyxe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2D_Layout_20250223_000345\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/el55jyxe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/wandb/run-20250223_000345-6ucsm303\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2D_Layout_20250223_000345\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/6ucsm303\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/wandb/run-20250223_000345-63niexqs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2D_Layout_20250223_000345\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/63niexqs\u001b[0m\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO:__main__:Generated 600 train, 100 val samples.\n",
      "INFO:__main__:Generated 600 train, 100 val samples.\n",
      "INFO:__main__:Generated 600 train, 100 val samples.\n",
      "INFO:__main__:Generated 600 train, 100 val samples.\n",
      "INFO:__main__:Generated 600 train, 100 val samples.\n",
      "INFO:__main__:Generated 600 train, 100 val samples.\n",
      "INFO:__main__:Generated 600 train, 100 val samples.\n",
      "INFO:__main__:Generated 600 train, 100 val samples.\n",
      "INFO:__main__:Starting training.\n",
      "INFO:__main__:Starting training.\n",
      "INFO:__main__:Starting training.\n",
      "INFO:__main__:Starting training.\n",
      "INFO:__main__:Starting training.\n",
      "INFO:__main__:Starting training.\n",
      "INFO:__main__:Starting training.\n",
      "INFO:__main__:Starting training.\n",
      "  0%|                                                   | 0/200 [00:00<?, ?it/s]/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "[rank7]:[W223 00:05:19.378988315 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank1]:[W223 00:05:19.381024357 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank2]:[W223 00:05:19.383135378 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank5]:[W223 00:05:19.383218481 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank3]:[W223 00:05:19.383891142 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank6]:[W223 00:05:19.384801454 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank4]:[W223 00:05:19.387794090 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[rank0]:[W223 00:05:19.392046039 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_1.txt\n",
      "  0%|▏                                        | 1/200 [01:17<4:17:42, 77.70s/it]INFO:__main__:Logging to wandb at step 1\n",
      "{'loss': 0.0016, 'grad_norm': 0.38671875, 'learning_rate': 8.333333333333333e-07, 'completion_length': 1085.5, 'rewards/calculate_batch_reward': 0.02604166604578495, 'reward': 0.02604166604578495, 'reward_std': 0.1041666641831398, 'kl': 0.0, 'clip_ratio': 0.0, 'epoch': 0.0}\n",
      "  0%|▏                                        | 1/200 [01:17<4:17:42, 77.70s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 10. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_2.txt\n",
      "  1%|▍                                        | 2/200 [02:33<4:11:49, 76.31s/it]INFO:__main__:Logging to wandb at step 2\n",
      "{'loss': 0.0186, 'grad_norm': 0.3984375, 'learning_rate': 1.6666666666666667e-06, 'completion_length': 1030.5625, 'rewards/calculate_batch_reward': 0.02604166604578495, 'reward': 0.02604166604578495, 'reward_std': 0.1041666641831398, 'kl': 0.0, 'clip_ratio': 0.0, 'epoch': 0.01}\n",
      "  1%|▍                                        | 2/200 [02:33<4:11:49, 76.31s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 19. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 19. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_3.txt\n",
      "  2%|▌                                        | 3/200 [03:46<4:06:59, 75.23s/it]INFO:__main__:Logging to wandb at step 3\n",
      "{'loss': 0.0, 'grad_norm': 0.0015411376953125, 'learning_rate': 2.5e-06, 'completion_length': 1134.03125, 'rewards/calculate_batch_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'kl': 0.00044182037527207285, 'clip_ratio': 0.0, 'epoch': 0.01}\n",
      "  2%|▌                                        | 3/200 [03:47<4:06:59, 75.23s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 28. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 28. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_4.txt\n",
      "  2%|▊                                        | 4/200 [05:04<4:08:54, 76.20s/it]INFO:__main__:Logging to wandb at step 4\n",
      "{'loss': 0.0, 'grad_norm': 0.00147247314453125, 'learning_rate': 3.3333333333333333e-06, 'completion_length': 1089.96875, 'rewards/calculate_batch_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'kl': 0.00044612743658944964, 'clip_ratio': 0.0, 'epoch': 0.01}\n",
      "  2%|▊                                        | 4/200 [05:04<4:08:54, 76.20s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 37. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 37. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_5.txt\n",
      "  2%|█                                        | 5/200 [06:14<4:00:05, 73.87s/it]INFO:__main__:Logging to wandb at step 5\n",
      "{'loss': 0.0286, 'grad_norm': 0.384765625, 'learning_rate': 4.166666666666667e-06, 'completion_length': 1093.0, 'rewards/calculate_batch_reward': 0.03125, 'reward': 0.03125, 'reward_std': 0.08539125323295593, 'kl': 0.00044278783025220037, 'clip_ratio': 0.0, 'epoch': 0.02}\n",
      "  2%|█                                        | 5/200 [06:14<4:00:05, 73.87s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 46. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 46. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_6.txt\n",
      "  3%|█▏                                       | 6/200 [07:29<4:00:09, 74.28s/it]INFO:__main__:Logging to wandb at step 6\n",
      "{'loss': 0.0, 'grad_norm': 0.001678466796875, 'learning_rate': 5e-06, 'completion_length': 1027.5625, 'rewards/calculate_batch_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'kl': 0.0004510095895966515, 'clip_ratio': 0.0, 'epoch': 0.02}\n",
      "  3%|█▏                                       | 6/200 [07:35<4:00:09, 74.28s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 55. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 55. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_7.txt\n",
      "  4%|█▍                                       | 7/200 [08:51<4:06:34, 76.65s/it]INFO:__main__:Logging to wandb at step 7\n",
      "{'loss': 0.0, 'grad_norm': 0.00151824951171875, 'learning_rate': 5.833333333333334e-06, 'completion_length': 1101.84375, 'rewards/calculate_batch_reward': 0.0, 'reward': 0.0, 'reward_std': 0.0, 'kl': 0.00044866613461636007, 'clip_ratio': 0.0, 'epoch': 0.02}\n",
      "  4%|█▍                                       | 7/200 [08:51<4:06:34, 76.65s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7 that is less than the current step 64. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7 that is less than the current step 64. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_8.txt\n",
      "  4%|█▋                                       | 8/200 [10:06<4:03:41, 76.15s/it]INFO:__main__:Logging to wandb at step 8\n",
      "{'loss': 0.0339, 'grad_norm': 0.3125, 'learning_rate': 6.666666666666667e-06, 'completion_length': 1073.71875, 'rewards/calculate_batch_reward': 0.02083333395421505, 'reward': 0.02083333395421505, 'reward_std': 0.0833333358168602, 'kl': 0.0004765668127220124, 'clip_ratio': 0.0, 'epoch': 0.03}\n",
      "  4%|█▋                                       | 8/200 [10:06<4:03:41, 76.15s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8 that is less than the current step 73. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8 that is less than the current step 73. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_9.txt\n",
      "  4%|█▊                                       | 9/200 [11:24<4:04:24, 76.78s/it]INFO:__main__:Logging to wandb at step 9\n",
      "{'loss': 0.0047, 'grad_norm': 0.412109375, 'learning_rate': 7.5e-06, 'completion_length': 1009.71875, 'rewards/calculate_batch_reward': 0.02083333395421505, 'reward': 0.02083333395421505, 'reward_std': 0.0833333358168602, 'kl': 0.0004915817407891154, 'clip_ratio': 0.0, 'epoch': 0.03}\n",
      "  4%|█▊                                       | 9/200 [11:27<4:04:24, 76.78s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9 that is less than the current step 82. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 9 that is less than the current step 82. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_10.txt\n",
      "  5%|██                                      | 10/200 [12:39<4:01:59, 76.42s/it]INFO:__main__:Logging to wandb at step 10\n",
      "{'loss': -0.0162, 'grad_norm': 0.423828125, 'learning_rate': 8.333333333333334e-06, 'completion_length': 1161.84375, 'rewards/calculate_batch_reward': 0.03125, 'reward': 0.03125, 'reward_std': 0.125, 'kl': 0.0005342965596355498, 'clip_ratio': 0.0, 'epoch': 0.03}\n",
      "  5%|██                                      | 10/200 [12:40<4:01:59, 76.42s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 91. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 91. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_11.txt\n",
      "  6%|██▏                                     | 11/200 [13:59<4:03:41, 77.36s/it]INFO:__main__:Logging to wandb at step 11\n",
      "{'loss': -0.0095, 'grad_norm': 0.447265625, 'learning_rate': 9.166666666666666e-06, 'completion_length': 1116.96875, 'rewards/calculate_batch_reward': 0.015625, 'reward': 0.015625, 'reward_std': 0.0625, 'kl': 0.0006071024981793016, 'clip_ratio': 0.0, 'epoch': 0.04}\n",
      "  6%|██▏                                     | 11/200 [14:04<4:03:41, 77.36s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11 that is less than the current step 100. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 11 that is less than the current step 100. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_12.txt\n",
      "  6%|██▍                                     | 12/200 [15:06<3:52:59, 74.36s/it]INFO:__main__:Logging to wandb at step 12\n",
      "{'loss': 0.026, 'grad_norm': 0.35546875, 'learning_rate': 1e-05, 'completion_length': 969.09375, 'rewards/calculate_batch_reward': 0.0677083358168602, 'reward': 0.0677083358168602, 'reward_std': 0.1466248333454132, 'kl': 0.0007054847374092788, 'clip_ratio': 0.0, 'epoch': 0.04}\n",
      "  6%|██▍                                     | 12/200 [15:06<3:52:59, 74.36s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 12 that is less than the current step 109. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 12 that is less than the current step 109. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_13.txt\n",
      "  6%|██▌                                     | 13/200 [16:23<3:53:38, 74.96s/it]INFO:__main__:Logging to wandb at step 13\n",
      "{'loss': 0.0434, 'grad_norm': 0.3515625, 'learning_rate': 1.0833333333333334e-05, 'completion_length': 1091.4375, 'rewards/calculate_batch_reward': 0.046875, 'reward': 0.046875, 'reward_std': 0.12898732721805573, 'kl': 0.0008097919053398073, 'clip_ratio': 0.0, 'epoch': 0.04}\n",
      "  6%|██▌                                     | 13/200 [16:23<3:53:38, 74.96s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13 that is less than the current step 118. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 13 that is less than the current step 118. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_14.txt\n",
      "  7%|██▊                                     | 14/200 [17:41<3:55:25, 75.94s/it]INFO:__main__:Logging to wandb at step 14\n",
      "{'loss': -0.024, 'grad_norm': 0.57421875, 'learning_rate': 1.1666666666666668e-05, 'completion_length': 1004.8125, 'rewards/calculate_batch_reward': 0.2031250074505806, 'reward': 0.2031250074505806, 'reward_std': 0.34228813648223877, 'kl': 0.0010462746722623706, 'clip_ratio': 0.0, 'epoch': 0.05}\n",
      "  7%|██▊                                     | 14/200 [17:41<3:55:25, 75.94s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 14 that is less than the current step 127. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 14 that is less than the current step 127. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_15.txt\n",
      "  8%|███                                     | 15/200 [18:44<3:41:55, 71.98s/it]INFO:__main__:Logging to wandb at step 15\n",
      "{'loss': 0.0262, 'grad_norm': 0.53125, 'learning_rate': 1.25e-05, 'completion_length': 954.15625, 'rewards/calculate_batch_reward': 0.244791679084301, 'reward': 0.244791679084301, 'reward_std': 0.34809665381908417, 'kl': 0.0016207328881137073, 'clip_ratio': 0.0, 'epoch': 0.05}\n",
      "  8%|███                                     | 15/200 [18:44<3:41:55, 71.98s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15 that is less than the current step 136. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 15 that is less than the current step 136. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_16.txt\n",
      "  8%|███▏                                    | 16/200 [20:15<3:58:36, 77.81s/it]INFO:__main__:Logging to wandb at step 16\n",
      "{'loss': 0.0701, 'grad_norm': 0.5234375, 'learning_rate': 1.3333333333333333e-05, 'completion_length': 1070.90625, 'rewards/calculate_batch_reward': 0.3645833432674408, 'reward': 0.3645833432674408, 'reward_std': 0.3552263677120209, 'kl': 0.001974724291358143, 'clip_ratio': 0.0, 'epoch': 0.05}\n",
      "  8%|███▏                                    | 16/200 [20:15<3:58:36, 77.81s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 16 that is less than the current step 145. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 16 that is less than the current step 145. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_17.txt\n",
      "  8%|███▍                                    | 17/200 [21:32<3:56:53, 77.67s/it]INFO:__main__:Logging to wandb at step 17\n",
      "{'loss': 0.1213, 'grad_norm': 0.498046875, 'learning_rate': 1.4166666666666668e-05, 'completion_length': 1222.40625, 'rewards/calculate_batch_reward': 0.2916666567325592, 'reward': 0.2916666567325592, 'reward_std': 0.373300164937973, 'kl': 0.001858245872426778, 'clip_ratio': 0.0, 'epoch': 0.06}\n",
      "  8%|███▍                                    | 17/200 [21:39<3:56:53, 77.67s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17 that is less than the current step 154. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 17 that is less than the current step 154. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_18.txt\n",
      "  9%|███▌                                    | 18/200 [22:46<3:51:37, 76.36s/it]INFO:__main__:Logging to wandb at step 18\n",
      "{'loss': 0.0281, 'grad_norm': 0.5625, 'learning_rate': 1.5e-05, 'completion_length': 1007.09375, 'rewards/calculate_batch_reward': 0.4479166716337204, 'reward': 0.4479166716337204, 'reward_std': 0.348232626914978, 'kl': 0.002658357610926032, 'clip_ratio': 0.0, 'epoch': 0.06}\n",
      "  9%|███▌                                    | 18/200 [22:46<3:51:37, 76.36s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 18 that is less than the current step 163. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 18 that is less than the current step 163. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_19.txt\n",
      " 10%|███▊                                    | 19/200 [24:00<3:48:40, 75.80s/it]INFO:__main__:Logging to wandb at step 19\n",
      "{'loss': 0.0261, 'grad_norm': 0.546875, 'learning_rate': 1.5833333333333333e-05, 'completion_length': 977.375, 'rewards/calculate_batch_reward': 0.4270833432674408, 'reward': 0.4270833432674408, 'reward_std': 0.3798357844352722, 'kl': 0.0033993059769272804, 'clip_ratio': 0.0, 'epoch': 0.06}\n",
      " 10%|███▊                                    | 19/200 [24:00<3:48:40, 75.80s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 19 that is less than the current step 172. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 19 that is less than the current step 172. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_20.txt\n",
      " 10%|████                                    | 20/200 [25:14<3:45:26, 75.15s/it]INFO:__main__:Logging to wandb at step 20\n",
      "{'loss': 0.0671, 'grad_norm': 0.5625, 'learning_rate': 1.6666666666666667e-05, 'completion_length': 1004.1875, 'rewards/calculate_batch_reward': 0.359375, 'reward': 0.359375, 'reward_std': 0.36065712571144104, 'kl': 0.0036385232815518975, 'clip_ratio': 0.0, 'epoch': 0.07}\n",
      " 10%|████                                    | 20/200 [25:14<3:45:26, 75.15s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 181. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 20 that is less than the current step 181. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_21.txt\n",
      " 10%|████▏                                   | 21/200 [26:31<3:45:59, 75.75s/it]INFO:__main__:Logging to wandb at step 21\n",
      "{'loss': 0.0362, 'grad_norm': 0.5703125, 'learning_rate': 1.75e-05, 'completion_length': 1001.4375, 'rewards/calculate_batch_reward': 0.4479166567325592, 'reward': 0.4479166567325592, 'reward_std': 0.3856413662433624, 'kl': 0.004013964906334877, 'clip_ratio': 0.0, 'epoch': 0.07}\n",
      " 10%|████▏                                   | 21/200 [26:31<3:45:59, 75.75s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 21 that is less than the current step 190. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 21 that is less than the current step 190. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_22.txt\n",
      " 11%|████▍                                   | 22/200 [27:47<3:45:23, 75.97s/it]INFO:__main__:Logging to wandb at step 22\n",
      "{'loss': 0.086, 'grad_norm': 0.53515625, 'learning_rate': 1.8333333333333333e-05, 'completion_length': 1080.09375, 'rewards/calculate_batch_reward': 0.4375, 'reward': 0.4375, 'reward_std': 0.3779641091823578, 'kl': 0.004260496469214559, 'clip_ratio': 0.0, 'epoch': 0.07}\n",
      " 11%|████▍                                   | 22/200 [27:48<3:45:23, 75.97s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 22 that is less than the current step 199. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 22 that is less than the current step 199. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_23.txt\n",
      " 12%|████▌                                   | 23/200 [29:04<3:44:23, 76.06s/it]INFO:__main__:Logging to wandb at step 23\n",
      "{'loss': 0.1073, 'grad_norm': 0.53515625, 'learning_rate': 1.9166666666666667e-05, 'completion_length': 1099.5625, 'rewards/calculate_batch_reward': 0.4427083432674408, 'reward': 0.4427083432674408, 'reward_std': 0.3448198586702347, 'kl': 0.005054650595411658, 'clip_ratio': 0.0, 'epoch': 0.08}\n",
      " 12%|████▌                                   | 23/200 [29:04<3:44:23, 76.06s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 23 that is less than the current step 208. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 23 that is less than the current step 208. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_24.txt\n",
      " 12%|████▊                                   | 24/200 [30:06<3:31:07, 71.97s/it]INFO:__main__:Logging to wandb at step 24\n",
      "{'loss': 0.001, 'grad_norm': 0.6015625, 'learning_rate': 2e-05, 'completion_length': 893.65625, 'rewards/calculate_batch_reward': 0.359375, 'reward': 0.359375, 'reward_std': 0.38560934364795685, 'kl': 0.007073891116306186, 'clip_ratio': 0.0, 'epoch': 0.08}\n",
      " 12%|████▊                                   | 24/200 [30:06<3:31:07, 71.97s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 24 that is less than the current step 217. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 24 that is less than the current step 217. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_25.txt\n",
      " 12%|█████                                   | 25/200 [31:25<3:36:03, 74.08s/it]INFO:__main__:Logging to wandb at step 25\n",
      "{'loss': 0.0559, 'grad_norm': 0.52734375, 'learning_rate': 2.0833333333333336e-05, 'completion_length': 1035.625, 'rewards/calculate_batch_reward': 0.4635417014360428, 'reward': 0.4635417014360428, 'reward_std': 0.324327290058136, 'kl': 0.006898878375068307, 'clip_ratio': 0.0, 'epoch': 0.08}\n",
      " 12%|█████                                   | 25/200 [31:25<3:36:03, 74.08s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 25 that is less than the current step 226. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 25 that is less than the current step 226. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_26.txt\n",
      " 13%|█████▏                                  | 26/200 [32:31<3:27:46, 71.65s/it]INFO:__main__:Logging to wandb at step 26\n",
      "{'loss': 0.0533, 'grad_norm': 0.5234375, 'learning_rate': 2.1666666666666667e-05, 'completion_length': 1027.5, 'rewards/calculate_batch_reward': 0.5260416567325592, 'reward': 0.5260416567325592, 'reward_std': 0.3112950474023819, 'kl': 0.008140585385262966, 'clip_ratio': 0.0, 'epoch': 0.09}\n",
      " 13%|█████▏                                  | 26/200 [32:31<3:27:46, 71.65s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 26 that is less than the current step 235. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 26 that is less than the current step 235. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_27.txt\n",
      " 14%|█████▍                                  | 27/200 [33:33<3:17:54, 68.64s/it]INFO:__main__:Logging to wandb at step 27\n",
      "{'loss': 0.0452, 'grad_norm': 0.58203125, 'learning_rate': 2.25e-05, 'completion_length': 917.15625, 'rewards/calculate_batch_reward': 0.5468750298023224, 'reward': 0.5468750298023224, 'reward_std': 0.3463260531425476, 'kl': 0.009241082239896059, 'clip_ratio': 0.0, 'epoch': 0.09}\n",
      " 14%|█████▍                                  | 27/200 [33:33<3:17:54, 68.64s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 244. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 27 that is less than the current step 244. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_28.txt\n",
      " 14%|█████▌                                  | 28/200 [34:31<3:08:08, 65.63s/it]INFO:__main__:Logging to wandb at step 28\n",
      "{'loss': -0.0468, 'grad_norm': 0.59765625, 'learning_rate': 2.3333333333333336e-05, 'completion_length': 900.53125, 'rewards/calculate_batch_reward': 0.473958358168602, 'reward': 0.473958358168602, 'reward_std': 0.3475050926208496, 'kl': 0.011377576272934675, 'clip_ratio': 0.0, 'epoch': 0.09}\n",
      " 14%|█████▌                                  | 28/200 [34:34<3:08:08, 65.63s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 28 that is less than the current step 253. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 28 that is less than the current step 253. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_29.txt\n",
      " 14%|█████▊                                  | 29/200 [35:32<3:02:32, 64.05s/it]INFO:__main__:Logging to wandb at step 29\n",
      "{'loss': 0.0439, 'grad_norm': 0.60546875, 'learning_rate': 2.4166666666666667e-05, 'completion_length': 919.3125, 'rewards/calculate_batch_reward': 0.552083358168602, 'reward': 0.552083358168602, 'reward_std': 0.3118458539247513, 'kl': 0.012108373455703259, 'clip_ratio': 0.0, 'epoch': 0.1}\n",
      " 14%|█████▊                                  | 29/200 [35:32<3:02:32, 64.05s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 29 that is less than the current step 262. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 29 that is less than the current step 262. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_30.txt\n",
      " 15%|██████                                  | 30/200 [36:37<3:02:34, 64.44s/it]INFO:__main__:Logging to wandb at step 30\n",
      "{'loss': 0.0655, 'grad_norm': 0.51953125, 'learning_rate': 2.5e-05, 'completion_length': 974.25, 'rewards/calculate_batch_reward': 0.5781250298023224, 'reward': 0.5781250298023224, 'reward_std': 0.304132342338562, 'kl': 0.014645017217844725, 'clip_ratio': 0.0, 'epoch': 0.1}\n",
      " 15%|██████                                  | 30/200 [36:37<3:02:34, 64.44s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 271. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 30 that is less than the current step 271. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_31.txt\n",
      " 16%|██████▏                                 | 31/200 [37:56<3:14:01, 68.89s/it]INFO:__main__:Logging to wandb at step 31\n",
      "{'loss': 0.0567, 'grad_norm': 0.60546875, 'learning_rate': 2.5833333333333336e-05, 'completion_length': 878.5, 'rewards/calculate_batch_reward': 0.6510416865348816, 'reward': 0.6510416865348816, 'reward_std': 0.28330978751182556, 'kl': 0.01735357753932476, 'clip_ratio': 0.0, 'epoch': 0.1}\n",
      " 16%|██████▏                                 | 31/200 [37:56<3:14:01, 68.89s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 31 that is less than the current step 280. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 31 that is less than the current step 280. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_32.txt\n",
      " 16%|██████▍                                 | 32/200 [39:08<3:15:10, 69.71s/it]INFO:__main__:Logging to wandb at step 32\n",
      "{'loss': 0.0995, 'grad_norm': 0.53125, 'learning_rate': 2.6666666666666667e-05, 'completion_length': 908.75, 'rewards/calculate_batch_reward': 0.5, 'reward': 0.5, 'reward_std': 0.36949749290943146, 'kl': 0.01912092510610819, 'clip_ratio': 0.0, 'epoch': 0.11}\n",
      " 16%|██████▍                                 | 32/200 [39:08<3:15:10, 69.71s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 32 that is less than the current step 289. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 32 that is less than the current step 289. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_33.txt\n",
      " 16%|██████▌                                 | 33/200 [39:58<2:57:40, 63.84s/it]INFO:__main__:Logging to wandb at step 33\n",
      "{'loss': -0.0576, 'grad_norm': 0.578125, 'learning_rate': 2.7500000000000004e-05, 'completion_length': 822.65625, 'rewards/calculate_batch_reward': 0.5937500298023224, 'reward': 0.5937500298023224, 'reward_std': 0.28610923886299133, 'kl': 0.024379953742027283, 'clip_ratio': 0.0, 'epoch': 0.11}\n",
      " 16%|██████▌                                 | 33/200 [39:58<2:57:40, 63.84s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 33 that is less than the current step 298. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 33 that is less than the current step 298. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_34.txt\n",
      " 17%|██████▊                                 | 34/200 [40:37<2:35:34, 56.23s/it]INFO:__main__:Logging to wandb at step 34\n",
      "{'loss': 0.0138, 'grad_norm': 0.6171875, 'learning_rate': 2.8333333333333335e-05, 'completion_length': 776.65625, 'rewards/calculate_batch_reward': 0.6666666865348816, 'reward': 0.6666666865348816, 'reward_std': 0.1842212788760662, 'kl': 0.02601455245167017, 'clip_ratio': 0.0, 'epoch': 0.11}\n",
      " 17%|██████▊                                 | 34/200 [40:37<2:35:34, 56.23s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 34 that is less than the current step 307. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 34 that is less than the current step 307. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_35.txt\n",
      " 18%|███████                                 | 35/200 [41:28<2:30:47, 54.83s/it]INFO:__main__:Logging to wandb at step 35\n",
      "{'loss': 0.0618, 'grad_norm': 0.5703125, 'learning_rate': 2.916666666666667e-05, 'completion_length': 837.40625, 'rewards/calculate_batch_reward': 0.6354166567325592, 'reward': 0.6354166567325592, 'reward_std': 0.2999332472681999, 'kl': 0.02814180590212345, 'clip_ratio': 0.0, 'epoch': 0.12}\n",
      " 18%|███████                                 | 35/200 [41:28<2:30:47, 54.83s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 35 that is less than the current step 316. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 35 that is less than the current step 316. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_36.txt\n",
      " 18%|███████▏                                | 36/200 [42:13<2:21:20, 51.71s/it]INFO:__main__:Logging to wandb at step 36\n",
      "{'loss': 0.0394, 'grad_norm': 0.625, 'learning_rate': 3e-05, 'completion_length': 825.1875, 'rewards/calculate_batch_reward': 0.6041666865348816, 'reward': 0.6041666865348816, 'reward_std': 0.2603294476866722, 'kl': 0.030899394303560257, 'clip_ratio': 0.0, 'epoch': 0.12}\n",
      " 18%|███████▏                                | 36/200 [42:13<2:21:20, 51.71s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 36 that is less than the current step 325. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 36 that is less than the current step 325. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_37.txt\n",
      " 18%|███████▍                                | 37/200 [42:50<2:08:45, 47.40s/it]INFO:__main__:Logging to wandb at step 37\n",
      "{'loss': -0.0032, 'grad_norm': 0.57421875, 'learning_rate': 3.0833333333333335e-05, 'completion_length': 776.9375, 'rewards/calculate_batch_reward': 0.7031249701976776, 'reward': 0.7031249701976776, 'reward_std': 0.2390543594956398, 'kl': 0.03318230248987675, 'clip_ratio': 0.0, 'epoch': 0.12}\n",
      " 18%|███████▍                                | 37/200 [42:50<2:08:45, 47.40s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 37 that is less than the current step 334. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 37 that is less than the current step 334. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_38.txt\n",
      " 19%|███████▌                                | 38/200 [43:25<1:58:19, 43.83s/it]INFO:__main__:Logging to wandb at step 38\n",
      "{'loss': 0.0083, 'grad_norm': 0.66015625, 'learning_rate': 3.1666666666666666e-05, 'completion_length': 779.59375, 'rewards/calculate_batch_reward': 0.6406250298023224, 'reward': 0.6406250298023224, 'reward_std': 0.2841627299785614, 'kl': 0.036091770976781845, 'clip_ratio': 0.0, 'epoch': 0.13}\n",
      " 19%|███████▌                                | 38/200 [43:25<1:58:19, 43.83s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 38 that is less than the current step 343. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 38 that is less than the current step 343. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_39.txt\n",
      " 20%|███████▊                                | 39/200 [44:11<1:59:13, 44.43s/it]INFO:__main__:Logging to wandb at step 39\n",
      "{'loss': 0.0156, 'grad_norm': 0.64453125, 'learning_rate': 3.2500000000000004e-05, 'completion_length': 770.125, 'rewards/calculate_batch_reward': 0.703125, 'reward': 0.703125, 'reward_std': 0.20719362050294876, 'kl': 0.03767665475606918, 'clip_ratio': 0.0, 'epoch': 0.13}\n",
      " 20%|███████▊                                | 39/200 [44:14<1:59:13, 44.43s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 39 that is less than the current step 352. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 39 that is less than the current step 352. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_40.txt\n",
      " 20%|████████                                | 40/200 [44:51<1:54:45, 43.03s/it]INFO:__main__:Logging to wandb at step 40\n",
      "{'loss': -0.0514, 'grad_norm': 0.6875, 'learning_rate': 3.3333333333333335e-05, 'completion_length': 717.625, 'rewards/calculate_batch_reward': 0.671875, 'reward': 0.671875, 'reward_std': 0.2575828805565834, 'kl': 0.042089492082595825, 'clip_ratio': 0.0, 'epoch': 0.13}\n",
      " 20%|████████                                | 40/200 [44:51<1:54:45, 43.03s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 361. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 361. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_41.txt\n",
      " 20%|████████▏                               | 41/200 [45:27<1:48:16, 40.86s/it]INFO:__main__:Logging to wandb at step 41\n",
      "{'loss': -0.0638, 'grad_norm': 0.671875, 'learning_rate': 3.4166666666666666e-05, 'completion_length': 700.625, 'rewards/calculate_batch_reward': 0.7187500298023224, 'reward': 0.7187500298023224, 'reward_std': 0.16082127764821053, 'kl': 0.04331187903881073, 'clip_ratio': 0.0, 'epoch': 0.14}\n",
      " 20%|████████▏                               | 41/200 [45:28<1:48:16, 40.86s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 41 that is less than the current step 370. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 41 that is less than the current step 370. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_42.txt\n",
      " 21%|████████▍                               | 42/200 [46:12<1:50:56, 42.13s/it]INFO:__main__:Logging to wandb at step 42\n",
      "{'loss': 0.0417, 'grad_norm': 0.5546875, 'learning_rate': 3.5e-05, 'completion_length': 820.0, 'rewards/calculate_batch_reward': 0.671875, 'reward': 0.671875, 'reward_std': 0.18198389559984207, 'kl': 0.039339909330010414, 'clip_ratio': 0.0, 'epoch': 0.14}\n",
      " 21%|████████▍                               | 42/200 [46:24<1:50:56, 42.13s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 42 that is less than the current step 379. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 42 that is less than the current step 379. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_43.txt\n",
      " 22%|████████▌                               | 43/200 [47:12<2:04:36, 47.62s/it]INFO:__main__:Logging to wandb at step 43\n",
      "{'loss': 0.005, 'grad_norm': 0.6328125, 'learning_rate': 3.5833333333333335e-05, 'completion_length': 762.15625, 'rewards/calculate_batch_reward': 0.7447916865348816, 'reward': 0.7447916865348816, 'reward_std': 0.1840546801686287, 'kl': 0.04337518475949764, 'clip_ratio': 0.0, 'epoch': 0.14}\n",
      " 22%|████████▌                               | 43/200 [47:12<2:04:36, 47.62s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 43 that is less than the current step 388. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 43 that is less than the current step 388. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_44.txt\n",
      " 22%|████████▊                               | 44/200 [47:51<1:56:26, 44.78s/it]INFO:__main__:Logging to wandb at step 44\n",
      "{'loss': 0.0207, 'grad_norm': 0.5625, 'learning_rate': 3.6666666666666666e-05, 'completion_length': 813.40625, 'rewards/calculate_batch_reward': 0.7291666269302368, 'reward': 0.7291666269302368, 'reward_std': 0.26252400130033493, 'kl': 0.04483317397534847, 'clip_ratio': 0.0, 'epoch': 0.15}\n",
      " 22%|████████▊                               | 44/200 [47:51<1:56:26, 44.78s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 44 that is less than the current step 397. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 44 that is less than the current step 397. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_45.txt\n",
      " 22%|█████████                               | 45/200 [48:35<1:55:12, 44.59s/it]INFO:__main__:Logging to wandb at step 45\n",
      "{'loss': -0.0306, 'grad_norm': 0.55078125, 'learning_rate': 3.7500000000000003e-05, 'completion_length': 804.375, 'rewards/calculate_batch_reward': 0.7343749701976776, 'reward': 0.7343749701976776, 'reward_std': 0.18849613517522812, 'kl': 0.050501976162195206, 'clip_ratio': 0.0, 'epoch': 0.15}\n",
      " 22%|█████████                               | 45/200 [48:43<1:55:12, 44.59s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 45 that is less than the current step 406. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 45 that is less than the current step 406. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_46.txt\n",
      " 23%|█████████▏                              | 46/200 [49:56<2:22:38, 55.58s/it]INFO:__main__:Logging to wandb at step 46\n",
      "{'loss': 0.0371, 'grad_norm': 0.6328125, 'learning_rate': 3.8333333333333334e-05, 'completion_length': 794.8125, 'rewards/calculate_batch_reward': 0.7604166865348816, 'reward': 0.7604166865348816, 'reward_std': 0.1805339828133583, 'kl': 0.050664087757468224, 'clip_ratio': 0.0, 'epoch': 0.15}\n",
      " 23%|█████████▏                              | 46/200 [49:56<2:22:38, 55.58s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 46 that is less than the current step 415. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 46 that is less than the current step 415. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_47.txt\n",
      " 24%|█████████▍                              | 47/200 [50:39<2:11:58, 51.75s/it]INFO:__main__:Logging to wandb at step 47\n",
      "{'loss': -0.0082, 'grad_norm': 0.5625, 'learning_rate': 3.9166666666666665e-05, 'completion_length': 902.5625, 'rewards/calculate_batch_reward': 0.75, 'reward': 0.75, 'reward_std': 0.2043413445353508, 'kl': 0.0505685992538929, 'clip_ratio': 0.0, 'epoch': 0.16}\n",
      " 24%|█████████▍                              | 47/200 [50:39<2:11:58, 51.75s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 47 that is less than the current step 424. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 47 that is less than the current step 424. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_48.txt\n",
      " 24%|█████████▌                              | 48/200 [51:19<2:02:17, 48.27s/it]INFO:__main__:Logging to wandb at step 48\n",
      "{'loss': -0.0186, 'grad_norm': 0.5625, 'learning_rate': 4e-05, 'completion_length': 827.46875, 'rewards/calculate_batch_reward': 0.8124999701976776, 'reward': 0.8124999701976776, 'reward_std': 0.1683202087879181, 'kl': 0.060451164841651917, 'clip_ratio': 0.0, 'epoch': 0.16}\n",
      " 24%|█████████▌                              | 48/200 [51:31<2:02:17, 48.27s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 48 that is less than the current step 433. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 48 that is less than the current step 433. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_49.txt\n",
      " 24%|█████████▊                              | 49/200 [52:11<2:04:41, 49.55s/it]INFO:__main__:Logging to wandb at step 49\n",
      "{'loss': 0.0148, 'grad_norm': 0.578125, 'learning_rate': 4.0833333333333334e-05, 'completion_length': 855.75, 'rewards/calculate_batch_reward': 0.6666666269302368, 'reward': 0.6666666269302368, 'reward_std': 0.33903267979621887, 'kl': 0.05774710886180401, 'clip_ratio': 0.0, 'epoch': 0.16}\n",
      " 24%|█████████▊                              | 49/200 [52:11<2:04:41, 49.55s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 49 that is less than the current step 442. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 49 that is less than the current step 442. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_50.txt\n",
      " 25%|██████████                              | 50/200 [53:01<2:03:39, 49.46s/it]INFO:__main__:Logging to wandb at step 50\n",
      "{'loss': 0.0457, 'grad_norm': 0.61328125, 'learning_rate': 4.166666666666667e-05, 'completion_length': 844.46875, 'rewards/calculate_batch_reward': 0.7916666567325592, 'reward': 0.7916666567325592, 'reward_std': 0.23458530753850937, 'kl': 0.06453249976038933, 'clip_ratio': 0.0, 'epoch': 0.17}\n",
      " 25%|██████████                              | 50/200 [53:01<2:03:39, 49.46s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 451. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 50 that is less than the current step 451. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_51.txt\n",
      " 26%|██████████▏                             | 51/200 [53:48<2:01:18, 48.85s/it]INFO:__main__:Logging to wandb at step 51\n",
      "{'loss': 0.0136, 'grad_norm': 0.5625, 'learning_rate': 4.25e-05, 'completion_length': 895.875, 'rewards/calculate_batch_reward': 0.8333333134651184, 'reward': 0.8333333134651184, 'reward_std': 0.14841293543577194, 'kl': 0.060076311230659485, 'clip_ratio': 0.0, 'epoch': 0.17}\n",
      " 26%|██████████▏                             | 51/200 [53:48<2:01:18, 48.85s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 51 that is less than the current step 460. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 51 that is less than the current step 460. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_52.txt\n",
      " 26%|██████████▍                             | 52/200 [54:41<2:03:26, 50.05s/it]INFO:__main__:Logging to wandb at step 52\n",
      "{'loss': -0.0065, 'grad_norm': 0.5859375, 'learning_rate': 4.3333333333333334e-05, 'completion_length': 901.0, 'rewards/calculate_batch_reward': 0.7916666567325592, 'reward': 0.7916666567325592, 'reward_std': 0.22622431814670563, 'kl': 0.06583864241838455, 'clip_ratio': 0.0, 'epoch': 0.17}\n",
      " 26%|██████████▍                             | 52/200 [54:41<2:03:26, 50.05s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 52 that is less than the current step 469. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 52 that is less than the current step 469. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_53.txt\n",
      " 26%|██████████▌                             | 53/200 [55:26<1:59:01, 48.58s/it]INFO:__main__:Logging to wandb at step 53\n",
      "{'loss': -0.022, 'grad_norm': 0.58984375, 'learning_rate': 4.4166666666666665e-05, 'completion_length': 888.34375, 'rewards/calculate_batch_reward': 0.8489583134651184, 'reward': 0.8489583134651184, 'reward_std': 0.18973080813884735, 'kl': 0.06591969728469849, 'clip_ratio': 0.0, 'epoch': 0.18}\n",
      " 26%|██████████▌                             | 53/200 [55:29<1:59:01, 48.58s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 53 that is less than the current step 478. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 53 that is less than the current step 478. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_54.txt\n",
      " 27%|██████████▊                             | 54/200 [56:07<1:52:40, 46.30s/it]INFO:__main__:Logging to wandb at step 54\n",
      "{'loss': 0.0057, 'grad_norm': 0.58984375, 'learning_rate': 4.5e-05, 'completion_length': 888.375, 'rewards/calculate_batch_reward': 0.7708333134651184, 'reward': 0.7708333134651184, 'reward_std': 0.22197026014328003, 'kl': 0.06930902600288391, 'clip_ratio': 0.0, 'epoch': 0.18}\n",
      " 27%|██████████▊                             | 54/200 [56:07<1:52:40, 46.30s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 487. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 54 that is less than the current step 487. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_55.txt\n",
      " 28%|███████████                             | 55/200 [56:51<1:50:04, 45.55s/it]INFO:__main__:Logging to wandb at step 55\n",
      "{'loss': 0.0336, 'grad_norm': 0.5859375, 'learning_rate': 4.5833333333333334e-05, 'completion_length': 889.0, 'rewards/calculate_batch_reward': 0.8125, 'reward': 0.8125, 'reward_std': 0.2534467875957489, 'kl': 0.06916392967104912, 'clip_ratio': 0.0, 'epoch': 0.18}\n",
      " 28%|███████████                             | 55/200 [57:01<1:50:04, 45.55s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 55 that is less than the current step 496. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 55 that is less than the current step 496. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_56.txt\n",
      " 28%|███████████▏                            | 56/200 [57:41<1:52:19, 46.80s/it]INFO:__main__:Logging to wandb at step 56\n",
      "{'loss': 0.001, 'grad_norm': 0.59375, 'learning_rate': 4.666666666666667e-05, 'completion_length': 878.59375, 'rewards/calculate_batch_reward': 0.8802083134651184, 'reward': 0.8802083134651184, 'reward_std': 0.1300419233739376, 'kl': 0.07602918520569801, 'clip_ratio': 0.0, 'epoch': 0.19}\n",
      " 28%|███████████▏                            | 56/200 [57:41<1:52:19, 46.80s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 56 that is less than the current step 505. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 56 that is less than the current step 505. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_57.txt\n",
      " 28%|███████████▍                            | 57/200 [58:40<2:00:15, 50.46s/it]INFO:__main__:Logging to wandb at step 57\n",
      "{'loss': 0.0273, 'grad_norm': 0.58203125, 'learning_rate': 4.75e-05, 'completion_length': 947.84375, 'rewards/calculate_batch_reward': 0.8281249701976776, 'reward': 0.8281249701976776, 'reward_std': 0.17046264186501503, 'kl': 0.06818288192152977, 'clip_ratio': 0.0, 'epoch': 0.19}\n",
      " 28%|███████████▍                            | 57/200 [58:40<2:00:15, 50.46s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 57 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 57 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_58.txt\n",
      " 29%|███████████▌                            | 58/200 [59:45<2:09:45, 54.83s/it]INFO:__main__:Logging to wandb at step 58\n",
      "{'loss': 0.0133, 'grad_norm': 0.5546875, 'learning_rate': 4.8333333333333334e-05, 'completion_length': 945.03125, 'rewards/calculate_batch_reward': 0.828125, 'reward': 0.828125, 'reward_std': 0.2159229964017868, 'kl': 0.07053669914603233, 'clip_ratio': 0.0, 'epoch': 0.19}\n",
      " 29%|███████████▌                            | 58/200 [59:45<2:09:45, 54.83s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 58 that is less than the current step 523. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 58 that is less than the current step 523. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_59.txt\n",
      " 30%|███████████▏                          | 59/200 [1:00:49<2:15:40, 57.74s/it]INFO:__main__:Logging to wandb at step 59\n",
      "{'loss': 0.095, 'grad_norm': 0.60546875, 'learning_rate': 4.9166666666666665e-05, 'completion_length': 976.8125, 'rewards/calculate_batch_reward': 0.8593749701976776, 'reward': 0.8593749701976776, 'reward_std': 0.20696565508842468, 'kl': 0.06932790204882622, 'clip_ratio': 0.0, 'epoch': 0.2}\n",
      " 30%|███████████▏                          | 59/200 [1:00:49<2:15:40, 57.74s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 59 that is less than the current step 532. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 59 that is less than the current step 532. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_60.txt\n",
      " 30%|███████████▍                          | 60/200 [1:01:56<2:21:24, 60.60s/it]INFO:__main__:Logging to wandb at step 60\n",
      "{'loss': 0.0753, 'grad_norm': 0.66015625, 'learning_rate': 5e-05, 'completion_length': 932.46875, 'rewards/calculate_batch_reward': 0.7604166269302368, 'reward': 0.7604166269302368, 'reward_std': 0.31894777715206146, 'kl': 0.07359741255640984, 'clip_ratio': 0.0, 'epoch': 0.2}\n",
      " 30%|███████████▍                          | 60/200 [1:01:57<2:21:24, 60.60s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 541. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 541. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_61.txt\n",
      " 30%|███████████▌                          | 61/200 [1:03:00<2:22:13, 61.39s/it]INFO:__main__:Logging to wandb at step 61\n",
      "{'loss': -0.0251, 'grad_norm': 0.51953125, 'learning_rate': 4.9993705873562665e-05, 'completion_length': 832.125, 'rewards/calculate_batch_reward': 0.8697916567325592, 'reward': 0.8697916567325592, 'reward_std': 0.17437925189733505, 'kl': 0.08294983208179474, 'clip_ratio': 0.0, 'epoch': 0.2}\n",
      " 30%|███████████▌                          | 61/200 [1:03:16<2:22:13, 61.39s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 61 that is less than the current step 550. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 61 that is less than the current step 550. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_62.txt\n",
      " 31%|███████████▊                          | 62/200 [1:03:47<2:11:43, 57.27s/it]INFO:__main__:Logging to wandb at step 62\n",
      "{'loss': -0.0254, 'grad_norm': 0.59375, 'learning_rate': 4.997482666353287e-05, 'completion_length': 748.0625, 'rewards/calculate_batch_reward': 0.84375, 'reward': 0.84375, 'reward_std': 0.2208244539797306, 'kl': 0.08300522714853287, 'clip_ratio': 0.0, 'epoch': 0.21}\n",
      " 31%|███████████▊                          | 62/200 [1:03:47<2:11:43, 57.27s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 62 that is less than the current step 559. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 62 that is less than the current step 559. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_63.txt\n",
      " 32%|███████████▉                          | 63/200 [1:04:29<2:00:19, 52.70s/it]INFO:__main__:Logging to wandb at step 63\n",
      "{'loss': -0.0087, 'grad_norm': 0.64453125, 'learning_rate': 4.99433718761614e-05, 'completion_length': 735.5625, 'rewards/calculate_batch_reward': 0.8645833134651184, 'reward': 0.8645833134651184, 'reward_std': 0.1968696564435959, 'kl': 0.08664428070187569, 'clip_ratio': 0.0, 'epoch': 0.21}\n",
      " 32%|███████████▉                          | 63/200 [1:04:29<2:00:19, 52.70s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 63 that is less than the current step 568. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 63 that is less than the current step 568. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_64.txt\n",
      " 32%|████████████▏                         | 64/200 [1:05:22<1:59:16, 52.62s/it]INFO:__main__:Logging to wandb at step 64\n",
      "{'loss': 0.056, 'grad_norm': 0.66015625, 'learning_rate': 4.989935734988098e-05, 'completion_length': 706.0625, 'rewards/calculate_batch_reward': 0.8333333134651184, 'reward': 0.8333333134651184, 'reward_std': 0.2913190871477127, 'kl': 0.09591549634933472, 'clip_ratio': 0.0, 'epoch': 0.21}\n",
      " 32%|████████████▏                         | 64/200 [1:05:34<1:59:16, 52.62s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 64 that is less than the current step 577. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 64 that is less than the current step 577. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_65.txt\n",
      " 32%|████████████▎                         | 65/200 [1:06:03<1:51:01, 49.35s/it]INFO:__main__:Logging to wandb at step 65\n",
      "{'loss': 0.0214, 'grad_norm': 0.640625, 'learning_rate': 4.984280524733107e-05, 'completion_length': 650.46875, 'rewards/calculate_batch_reward': 0.9166666567325592, 'reward': 0.9166666567325592, 'reward_std': 0.1782601922750473, 'kl': 0.09190740436315536, 'clip_ratio': 0.0, 'epoch': 0.22}\n",
      " 32%|████████████▎                         | 65/200 [1:06:04<1:51:01, 49.35s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 65 that is less than the current step 586. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 65 that is less than the current step 586. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_66.txt\n",
      " 33%|████████████▌                         | 66/200 [1:06:46<1:45:40, 47.31s/it]INFO:__main__:Logging to wandb at step 66\n",
      "{'loss': 0.0476, 'grad_norm': 0.5703125, 'learning_rate': 4.977374404419837e-05, 'completion_length': 700.4375, 'rewards/calculate_batch_reward': 0.875, 'reward': 0.875, 'reward_std': 0.19236673414707184, 'kl': 0.08860808238387108, 'clip_ratio': 0.0, 'epoch': 0.22}\n",
      " 33%|████████████▌                         | 66/200 [1:06:46<1:45:40, 47.31s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 66 that is less than the current step 595. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 66 that is less than the current step 595. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_67.txt\n",
      " 34%|████████████▋                         | 67/200 [1:07:22<1:37:25, 43.95s/it]INFO:__main__:Logging to wandb at step 67\n",
      "{'loss': -0.0022, 'grad_norm': 0.57421875, 'learning_rate': 4.9692208514878444e-05, 'completion_length': 655.03125, 'rewards/calculate_batch_reward': 0.8854166567325592, 'reward': 0.8854166567325592, 'reward_std': 0.17591559886932373, 'kl': 0.09074766933917999, 'clip_ratio': 0.0, 'epoch': 0.22}\n",
      " 34%|████████████▋                         | 67/200 [1:07:22<1:37:25, 43.95s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 67 that is less than the current step 604. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 67 that is less than the current step 604. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_68.txt\n",
      " 34%|████████████▉                         | 68/200 [1:08:16<1:43:18, 46.95s/it]INFO:__main__:Logging to wandb at step 68\n",
      "{'loss': 0.0482, 'grad_norm': 0.7421875, 'learning_rate': 4.959823971496574e-05, 'completion_length': 720.875, 'rewards/calculate_batch_reward': 0.8437499701976776, 'reward': 0.8437499701976776, 'reward_std': 0.2428511306643486, 'kl': 0.10331760346889496, 'clip_ratio': 0.0, 'epoch': 0.23}\n",
      " 34%|████████████▉                         | 68/200 [1:08:16<1:43:18, 46.95s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 68 that is less than the current step 613. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 68 that is less than the current step 613. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_69.txt\n",
      " 34%|█████████████                         | 69/200 [1:08:47<1:32:14, 42.24s/it]INFO:__main__:Logging to wandb at step 69\n",
      "{'loss': 0.0078, 'grad_norm': 0.65234375, 'learning_rate': 4.9491884960580894e-05, 'completion_length': 646.9375, 'rewards/calculate_batch_reward': 0.8489583134651184, 'reward': 0.8489583134651184, 'reward_std': 0.24554280936717987, 'kl': 0.09523540362715721, 'clip_ratio': 0.0, 'epoch': 0.23}\n",
      " 34%|█████████████                         | 69/200 [1:08:53<1:32:14, 42.24s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 69 that is less than the current step 622. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 69 that is less than the current step 622. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_70.txt\n",
      " 35%|█████████████▎                        | 70/200 [1:09:26<1:29:00, 41.08s/it]INFO:__main__:Logging to wandb at step 70\n",
      "{'loss': 0.026, 'grad_norm': 0.64453125, 'learning_rate': 4.937319780454559e-05, 'completion_length': 672.96875, 'rewards/calculate_batch_reward': 0.9322916567325592, 'reward': 0.9322916567325592, 'reward_std': 0.09429056569933891, 'kl': 0.10218803212046623, 'clip_ratio': 0.0, 'epoch': 0.23}\n",
      " 35%|█████████████▎                        | 70/200 [1:09:32<1:29:00, 41.08s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 631. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 70 that is less than the current step 631. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_71.txt\n",
      " 36%|█████████████▍                        | 71/200 [1:10:08<1:29:17, 41.53s/it]INFO:__main__:Logging to wandb at step 71\n",
      "{'loss': -0.0125, 'grad_norm': 0.5703125, 'learning_rate': 4.9242238009417175e-05, 'completion_length': 670.5625, 'rewards/calculate_batch_reward': 0.8177083432674408, 'reward': 0.8177083432674408, 'reward_std': 0.24042347073554993, 'kl': 0.1056562028825283, 'clip_ratio': 0.0, 'epoch': 0.24}\n",
      " 36%|█████████████▍                        | 71/200 [1:10:08<1:29:17, 41.53s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 71 that is less than the current step 640. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 71 that is less than the current step 640. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_72.txt\n",
      " 36%|█████████████▋                        | 72/200 [1:11:02<1:36:34, 45.27s/it]INFO:__main__:Logging to wandb at step 72\n",
      "{'loss': 0.0723, 'grad_norm': 0.74609375, 'learning_rate': 4.909907151739633e-05, 'completion_length': 690.0, 'rewards/calculate_batch_reward': 0.8020833432674408, 'reward': 0.8020833432674408, 'reward_std': 0.2865549847483635, 'kl': 0.10521136224269867, 'clip_ratio': 0.0, 'epoch': 0.24}\n",
      " 36%|█████████████▋                        | 72/200 [1:11:02<1:36:34, 45.27s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 72 that is less than the current step 649. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 72 that is less than the current step 649. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_75.txt\n",
      " 38%|██████████████▎                       | 75/200 [1:13:44<1:51:13, 53.39s/it]INFO:__main__:Logging to wandb at step 75\n",
      "{'loss': 0.0646, 'grad_norm': 0.76953125, 'learning_rate': 4.8597083257709194e-05, 'completion_length': 783.625, 'rewards/calculate_batch_reward': 0.7343749701976776, 'reward': 0.7343749701976776, 'reward_std': 0.34409405291080475, 'kl': 0.1403086856007576, 'clip_ratio': 0.0, 'epoch': 0.25}\n",
      " 38%|██████████████▎                       | 75/200 [1:14:01<1:51:13, 53.39s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 75 that is less than the current step 676. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 75 that is less than the current step 676. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_76.txt\n",
      " 38%|██████████████▍                       | 76/200 [1:15:10<2:10:30, 63.15s/it]INFO:__main__:Logging to wandb at step 76\n",
      "{'loss': 0.0671, 'grad_norm': 0.65625, 'learning_rate': 4.8405871765993433e-05, 'completion_length': 725.90625, 'rewards/calculate_batch_reward': 0.9010416567325592, 'reward': 0.9010416567325592, 'reward_std': 0.16769562661647797, 'kl': 0.14344309270381927, 'clip_ratio': 0.0, 'epoch': 0.25}\n",
      " 38%|██████████████▍                       | 76/200 [1:15:12<2:10:30, 63.15s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 76 that is less than the current step 685. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 76 that is less than the current step 685. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_77.txt\n",
      " 38%|██████████████▋                       | 77/200 [1:16:06<2:05:02, 61.00s/it]INFO:__main__:Logging to wandb at step 77\n",
      "{'loss': 0.0392, 'grad_norm': 0.640625, 'learning_rate': 4.820287471297598e-05, 'completion_length': 746.46875, 'rewards/calculate_batch_reward': 0.6979166567325592, 'reward': 0.6979166567325592, 'reward_std': 0.3816095441579819, 'kl': 0.16159531474113464, 'clip_ratio': 0.0, 'epoch': 0.26}\n",
      " 38%|██████████████▋                       | 77/200 [1:16:06<2:05:02, 61.00s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 77 that is less than the current step 694. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 77 that is less than the current step 694. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_78.txt\n",
      " 39%|██████████████▊                       | 78/200 [1:16:58<1:59:00, 58.53s/it]INFO:__main__:Logging to wandb at step 78\n",
      "{'loss': 0.0647, 'grad_norm': 0.75390625, 'learning_rate': 4.7988194313786275e-05, 'completion_length': 713.3125, 'rewards/calculate_batch_reward': 0.7604166269302368, 'reward': 0.7604166269302368, 'reward_std': 0.3066142424941063, 'kl': 0.1677757129073143, 'clip_ratio': 0.0, 'epoch': 0.26}\n",
      " 39%|██████████████▊                       | 78/200 [1:16:59<1:59:00, 58.53s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 78 that is less than the current step 703. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 78 that is less than the current step 703. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_79.txt\n",
      " 40%|███████████████                       | 79/200 [1:17:39<1:47:11, 53.15s/it]INFO:__main__:Logging to wandb at step 79\n",
      "{'loss': -0.0079, 'grad_norm': 0.71484375, 'learning_rate': 4.7761938666470403e-05, 'completion_length': 685.5625, 'rewards/calculate_batch_reward': 0.78125, 'reward': 0.78125, 'reward_std': 0.3050774112343788, 'kl': 0.18833871185779572, 'clip_ratio': 0.0, 'epoch': 0.26}\n",
      " 40%|███████████████                       | 79/200 [1:17:39<1:47:11, 53.15s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 79 that is less than the current step 712. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 79 that is less than the current step 712. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_80.txt\n",
      " 40%|███████████████▏                      | 80/200 [1:18:12<1:34:18, 47.16s/it]INFO:__main__:Logging to wandb at step 80\n",
      "{'loss': 0.0232, 'grad_norm': 0.640625, 'learning_rate': 4.752422169756048e-05, 'completion_length': 638.78125, 'rewards/calculate_batch_reward': 0.8072916567325592, 'reward': 0.8072916567325592, 'reward_std': 0.2866727411746979, 'kl': 0.18809055536985397, 'clip_ratio': 0.0, 'epoch': 0.27}\n",
      " 40%|███████████████▏                      | 80/200 [1:18:12<1:34:18, 47.16s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 721. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 721. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_81.txt\n",
      " 40%|███████████████▍                      | 81/200 [1:19:20<1:45:38, 53.26s/it]INFO:__main__:Logging to wandb at step 81\n",
      "{'loss': -0.0141, 'grad_norm': 0.76171875, 'learning_rate': 4.72751631047092e-05, 'completion_length': 741.65625, 'rewards/calculate_batch_reward': 0.7864583134651184, 'reward': 0.7864583134651184, 'reward_std': 0.22890683263540268, 'kl': 0.2019568756222725, 'clip_ratio': 0.0, 'epoch': 0.27}\n",
      " 40%|███████████████▍                      | 81/200 [1:19:20<1:45:38, 53.26s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 730. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 81 that is less than the current step 730. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_82.txt\n",
      " 41%|███████████████▌                      | 82/200 [1:19:51<1:31:54, 46.73s/it]INFO:__main__:Logging to wandb at step 82\n",
      "{'loss': 0.0221, 'grad_norm': 0.6875, 'learning_rate': 4.701488829641845e-05, 'completion_length': 682.3125, 'rewards/calculate_batch_reward': 0.765625, 'reward': 0.765625, 'reward_std': 0.33530524373054504, 'kl': 0.16767912358045578, 'clip_ratio': 0.0, 'epoch': 0.27}\n",
      " 41%|███████████████▌                      | 82/200 [1:20:06<1:31:54, 46.73s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 82 that is less than the current step 739. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 82 that is less than the current step 739. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_83.txt\n",
      " 42%|███████████████▊                      | 83/200 [1:20:39<1:31:40, 47.01s/it]INFO:__main__:Logging to wandb at step 83\n",
      "{'loss': -0.0051, 'grad_norm': 0.63671875, 'learning_rate': 4.674352832889239e-05, 'completion_length': 691.0, 'rewards/calculate_batch_reward': 0.8958333730697632, 'reward': 0.8958333730697632, 'reward_std': 0.19409514218568802, 'kl': 0.15354960411787033, 'clip_ratio': 0.0, 'epoch': 0.28}\n",
      " 42%|███████████████▊                      | 83/200 [1:20:41<1:31:40, 47.01s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 83 that is less than the current step 748. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 83 that is less than the current step 748. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_84.txt\n",
      " 42%|███████████████▉                      | 84/200 [1:21:16<1:25:11, 44.07s/it]INFO:__main__:Logging to wandb at step 84\n",
      "{'loss': 0.0292, 'grad_norm': 0.5859375, 'learning_rate': 4.6461219840046654e-05, 'completion_length': 645.59375, 'rewards/calculate_batch_reward': 0.8541666567325592, 'reward': 0.8541666567325592, 'reward_std': 0.25706158578395844, 'kl': 0.13485722988843918, 'clip_ratio': 0.0, 'epoch': 0.28}\n",
      " 42%|███████████████▉                      | 84/200 [1:21:16<1:25:11, 44.07s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 84 that is less than the current step 757. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 84 that is less than the current step 757. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_85.txt\n",
      " 42%|████████████████▏                     | 85/200 [1:22:07<1:28:24, 46.12s/it]INFO:__main__:Logging to wandb at step 85\n",
      "{'loss': 0.0011, 'grad_norm': 0.796875, 'learning_rate': 4.6168104980707107e-05, 'completion_length': 570.6875, 'rewards/calculate_batch_reward': 0.7552083134651184, 'reward': 0.7552083134651184, 'reward_std': 0.35385607182979584, 'kl': 0.1343439593911171, 'clip_ratio': 0.0, 'epoch': 0.28}\n",
      " 42%|████████████████▏                     | 85/200 [1:22:24<1:28:24, 46.12s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 85 that is less than the current step 766. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 85 that is less than the current step 766. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_86.txt\n",
      " 43%|████████████████▎                     | 86/200 [1:22:51<1:26:34, 45.57s/it]INFO:__main__:Logging to wandb at step 86\n",
      "{'loss': -0.1129, 'grad_norm': 0.8046875, 'learning_rate': 4.586433134303257e-05, 'completion_length': 476.03125, 'rewards/calculate_batch_reward': 0.8489583432674408, 'reward': 0.8489583432674408, 'reward_std': 0.2594200074672699, 'kl': 0.23397862911224365, 'clip_ratio': 0.0, 'epoch': 0.29}\n",
      " 43%|████████████████▎                     | 86/200 [1:22:51<1:26:34, 45.57s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 86 that is less than the current step 775. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 86 that is less than the current step 775. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_87.txt\n",
      " 44%|████████████████▌                     | 87/200 [1:23:24<1:18:22, 41.61s/it]INFO:__main__:Logging to wandb at step 87\n",
      "{'loss': -0.0118, 'grad_norm': 0.5625, 'learning_rate': 4.5550051886197754e-05, 'completion_length': 589.21875, 'rewards/calculate_batch_reward': 0.9583333432674408, 'reward': 0.9583333432674408, 'reward_std': 0.08538032323122025, 'kl': 0.13544369488954544, 'clip_ratio': 0.0, 'epoch': 0.29}\n",
      " 44%|████████████████▌                     | 87/200 [1:23:24<1:18:22, 41.61s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 87 that is less than the current step 784. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 87 that is less than the current step 784. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_88.txt\n",
      " 44%|████████████████▋                     | 88/200 [1:24:04<1:17:10, 41.34s/it]INFO:__main__:Logging to wandb at step 88\n",
      "{'loss': 0.033, 'grad_norm': 0.65234375, 'learning_rate': 4.522542485937369e-05, 'completion_length': 756.34375, 'rewards/calculate_batch_reward': 0.9270833134651184, 'reward': 0.9270833134651184, 'reward_std': 0.1622678004205227, 'kl': 0.13477491587400436, 'clip_ratio': 0.0, 'epoch': 0.29}\n",
      " 44%|████████████████▋                     | 88/200 [1:24:13<1:17:10, 41.34s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 88 that is less than the current step 793. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 88 that is less than the current step 793. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_89.txt\n",
      " 44%|████████████████▉                     | 89/200 [1:24:52<1:20:01, 43.26s/it]INFO:__main__:Logging to wandb at step 89\n",
      "{'loss': 0.0135, 'grad_norm': 0.66015625, 'learning_rate': 4.489061372204453e-05, 'completion_length': 694.5, 'rewards/calculate_batch_reward': 0.9270833134651184, 'reward': 0.9270833134651184, 'reward_std': 0.11079317703843117, 'kl': 0.1538035199046135, 'clip_ratio': 0.0, 'epoch': 0.3}\n",
      " 44%|████████████████▉                     | 89/200 [1:24:52<1:20:01, 43.26s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 89 that is less than the current step 802. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 89 that is less than the current step 802. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_90.txt\n",
      " 45%|█████████████████                     | 90/200 [1:25:37<1:20:17, 43.79s/it]INFO:__main__:Logging to wandb at step 90\n",
      "{'loss': 0.0107, 'grad_norm': 0.75390625, 'learning_rate': 4.454578706170075e-05, 'completion_length': 715.0625, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.17601489275693893, 'kl': 0.14936795830726624, 'clip_ratio': 0.0, 'epoch': 0.3}\n",
      " 45%|█████████████████                     | 90/200 [1:25:50<1:20:17, 43.79s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 811. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 90 that is less than the current step 811. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_91.txt\n",
      " 46%|█████████████████▎                    | 91/200 [1:26:52<1:36:22, 53.05s/it]INFO:__main__:Logging to wandb at step 91\n",
      "{'loss': 0.0236, 'grad_norm': 0.59765625, 'learning_rate': 4.419111850895028e-05, 'completion_length': 685.5, 'rewards/calculate_batch_reward': 0.9479166269302368, 'reward': 0.9479166269302368, 'reward_std': 0.07978560775518417, 'kl': 0.1415308713912964, 'clip_ratio': 0.0, 'epoch': 0.3}\n",
      " 46%|█████████████████▎                    | 91/200 [1:26:52<1:36:22, 53.05s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 91 that is less than the current step 820. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 91 that is less than the current step 820. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_92.txt\n",
      " 46%|█████████████████▍                    | 92/200 [1:27:24<1:24:25, 46.90s/it]INFO:__main__:Logging to wandb at step 92\n",
      "{'loss': 0.0231, 'grad_norm': 0.609375, 'learning_rate': 4.382678665009028e-05, 'completion_length': 701.9375, 'rewards/calculate_batch_reward': 0.9218749701976776, 'reward': 0.9218749701976776, 'reward_std': 0.1648927927017212, 'kl': 0.15118441730737686, 'clip_ratio': 0.0, 'epoch': 0.31}\n",
      " 46%|█████████████████▍                    | 92/200 [1:27:26<1:24:25, 46.90s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 92 that is less than the current step 829. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 92 that is less than the current step 829. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_93.txt\n",
      " 46%|█████████████████▋                    | 93/200 [1:27:59<1:17:00, 43.18s/it]INFO:__main__:Logging to wandb at step 93\n",
      "{'loss': -0.0132, 'grad_norm': 0.5859375, 'learning_rate': 4.345297493718352e-05, 'completion_length': 704.875, 'rewards/calculate_batch_reward': 0.9166666567325592, 'reward': 0.9166666567325592, 'reward_std': 0.19317515194416046, 'kl': 0.1537325531244278, 'clip_ratio': 0.0, 'epoch': 0.31}\n",
      " 46%|█████████████████▋                    | 93/200 [1:27:59<1:17:00, 43.18s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 93 that is less than the current step 838. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 93 that is less than the current step 838. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_94.txt\n",
      " 47%|█████████████████▊                    | 94/200 [1:28:30<1:10:08, 39.70s/it]INFO:__main__:Logging to wandb at step 94\n",
      "{'loss': 0.0128, 'grad_norm': 0.57421875, 'learning_rate': 4.306987159568479e-05, 'completion_length': 664.1875, 'rewards/calculate_batch_reward': 0.9583333134651184, 'reward': 0.9583333134651184, 'reward_std': 0.07348554953932762, 'kl': 0.1407957524061203, 'clip_ratio': 0.0, 'epoch': 0.31}\n",
      " 47%|█████████████████▊                    | 94/200 [1:28:31<1:10:08, 39.70s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 94 that is less than the current step 847. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 94 that is less than the current step 847. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_95.txt\n",
      " 48%|██████████████████                    | 95/200 [1:29:05<1:06:51, 38.20s/it]INFO:__main__:Logging to wandb at step 95\n",
      "{'loss': -0.0066, 'grad_norm': 0.5859375, 'learning_rate': 4.267766952966369e-05, 'completion_length': 719.0625, 'rewards/calculate_batch_reward': 0.953125, 'reward': 0.953125, 'reward_std': 0.08518766984343529, 'kl': 0.1592242568731308, 'clip_ratio': 0.0, 'epoch': 0.32}\n",
      " 48%|██████████████████                    | 95/200 [1:29:05<1:06:51, 38.20s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 95 that is less than the current step 856. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 95 that is less than the current step 856. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_96.txt\n",
      " 48%|██████████████████▏                   | 96/200 [1:29:51<1:10:02, 40.41s/it]INFO:__main__:Logging to wandb at step 96\n",
      "{'loss': 0.0167, 'grad_norm': 0.392578125, 'learning_rate': 4.227656622467162e-05, 'completion_length': 727.9375, 'rewards/calculate_batch_reward': 0.9791666567325592, 'reward': 0.9791666567325592, 'reward_std': 0.03726780414581299, 'kl': 0.14336178451776505, 'clip_ratio': 0.0, 'epoch': 0.32}\n",
      " 48%|██████████████████▏                   | 96/200 [1:29:51<1:10:02, 40.41s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 96 that is less than the current step 865. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 96 that is less than the current step 865. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_97.txt\n",
      " 48%|██████████████████▍                   | 97/200 [1:30:29<1:08:17, 39.79s/it]INFO:__main__:Logging to wandb at step 97\n",
      "{'loss': 0.012, 'grad_norm': 0.62890625, 'learning_rate': 4.186676364830186e-05, 'completion_length': 699.09375, 'rewards/calculate_batch_reward': 0.9375, 'reward': 0.9375, 'reward_std': 0.10897057875990868, 'kl': 0.14654599875211716, 'clip_ratio': 0.0, 'epoch': 0.32}\n",
      " 48%|██████████████████▍                   | 97/200 [1:30:29<1:08:17, 39.79s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 97 that is less than the current step 874. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 97 that is less than the current step 874. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_98.txt\n",
      " 49%|██████████████████▌                   | 98/200 [1:31:10<1:08:19, 40.19s/it]INFO:__main__:Logging to wandb at step 98\n",
      "{'loss': 0.0292, 'grad_norm': 0.57421875, 'learning_rate': 4.144846814849282e-05, 'completion_length': 724.28125, 'rewards/calculate_batch_reward': 0.9687499701976776, 'reward': 0.9687499701976776, 'reward_std': 0.06573156453669071, 'kl': 0.14428997039794922, 'clip_ratio': 0.0, 'epoch': 0.33}\n",
      " 49%|██████████████████▌                   | 98/200 [1:31:10<1:08:19, 40.19s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 98 that is less than the current step 883. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 98 that is less than the current step 883. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_99.txt\n",
      " 50%|██████████████████▊                   | 99/200 [1:31:44<1:04:14, 38.16s/it]INFO:__main__:Logging to wandb at step 99\n",
      "{'loss': -0.0109, 'grad_norm': 0.6015625, 'learning_rate': 4.10218903496256e-05, 'completion_length': 681.03125, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.17968181520700455, 'kl': 0.14572666585445404, 'clip_ratio': 0.0, 'epoch': 0.33}\n",
      " 50%|██████████████████▊                   | 99/200 [1:31:44<1:04:14, 38.16s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 99 that is less than the current step 892. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 99 that is less than the current step 892. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_100.txt\n",
      " 50%|██████████████████▌                  | 100/200 [1:32:32<1:08:41, 41.22s/it]INFO:__main__:Logging to wandb at step 100\n",
      "{'loss': -0.0091, 'grad_norm': 0.58203125, 'learning_rate': 4.058724504646834e-05, 'completion_length': 712.09375, 'rewards/calculate_batch_reward': 0.9010416567325592, 'reward': 0.9010416567325592, 'reward_std': 0.17603270709514618, 'kl': 0.14349472522735596, 'clip_ratio': 0.0, 'epoch': 0.33}\n",
      " 50%|██████████████████▌                  | 100/200 [1:32:32<1:08:41, 41.22s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 901. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 100 that is less than the current step 901. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_101.txt\n",
      " 50%|██████████████████▋                  | 101/200 [1:33:06<1:04:12, 38.92s/it]INFO:__main__:Logging to wandb at step 101\n",
      "{'loss': 0.0217, 'grad_norm': 0.6328125, 'learning_rate': 4.01447510960205e-05, 'completion_length': 693.96875, 'rewards/calculate_batch_reward': 0.921875, 'reward': 0.921875, 'reward_std': 0.16701342537999153, 'kl': 0.14553195238113403, 'clip_ratio': 0.0, 'epoch': 0.34}\n",
      " 50%|██████████████████▋                  | 101/200 [1:33:06<1:04:12, 38.92s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 101 that is less than the current step 910. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 101 that is less than the current step 910. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_102.txt\n",
      " 51%|██████████████████▊                  | 102/200 [1:33:40<1:01:27, 37.63s/it]INFO:__main__:Logging to wandb at step 102\n",
      "{'loss': 0.0085, 'grad_norm': 0.59765625, 'learning_rate': 3.969463130731183e-05, 'completion_length': 712.21875, 'rewards/calculate_batch_reward': 0.9531249701976776, 'reward': 0.9531249701976776, 'reward_std': 0.07716060057282448, 'kl': 0.1467861458659172, 'clip_ratio': 0.0, 'epoch': 0.34}\n",
      " 51%|██████████████████▊                  | 102/200 [1:33:40<1:01:27, 37.63s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 102 that is less than the current step 919. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 102 that is less than the current step 919. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_103.txt\n",
      " 52%|████████████████████                   | 103/200 [1:34:13<58:42, 36.31s/it]INFO:__main__:Logging to wandb at step 103\n",
      "{'loss': 0.0087, 'grad_norm': 0.59765625, 'learning_rate': 3.92371123292113e-05, 'completion_length': 712.25, 'rewards/calculate_batch_reward': 0.9218749701976776, 'reward': 0.9218749701976776, 'reward_std': 0.19263660162687302, 'kl': 0.13953305780887604, 'clip_ratio': 0.0, 'epoch': 0.34}\n",
      " 52%|████████████████████                   | 103/200 [1:34:13<58:42, 36.31s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 103 that is less than the current step 928. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 103 that is less than the current step 928. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_104.txt\n",
      " 52%|████████████████████▎                  | 104/200 [1:34:50<58:08, 36.34s/it]INFO:__main__:Logging to wandb at step 104\n",
      "{'loss': 0.0042, 'grad_norm': 0.60546875, 'learning_rate': 3.8772424536302564e-05, 'completion_length': 680.71875, 'rewards/calculate_batch_reward': 0.9322916567325592, 'reward': 0.9322916567325592, 'reward_std': 0.12245376780629158, 'kl': 0.14538046717643738, 'clip_ratio': 0.0, 'epoch': 0.35}\n",
      " 52%|████████████████████▎                  | 104/200 [1:34:53<58:08, 36.34s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 104 that is less than the current step 937. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 104 that is less than the current step 937. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_105.txt\n",
      " 52%|████████████████████▍                  | 105/200 [1:35:24<56:25, 35.64s/it]INFO:__main__:Logging to wandb at step 105\n",
      "{'loss': 0.0175, 'grad_norm': 0.65234375, 'learning_rate': 3.830080191288342e-05, 'completion_length': 635.8125, 'rewards/calculate_batch_reward': 0.8906249701976776, 'reward': 0.8906249701976776, 'reward_std': 0.2530868798494339, 'kl': 0.14026211202144623, 'clip_ratio': 0.0, 'epoch': 0.35}\n",
      " 52%|████████████████████▍                  | 105/200 [1:35:24<56:25, 35.64s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 105 that is less than the current step 946. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 105 that is less than the current step 946. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_106.txt\n",
      " 53%|███████████████████▌                 | 106/200 [1:36:30<1:10:09, 44.79s/it]INFO:__main__:Logging to wandb at step 106\n",
      "{'loss': 0.0029, 'grad_norm': 0.6484375, 'learning_rate': 3.782248193514766e-05, 'completion_length': 737.78125, 'rewards/calculate_batch_reward': 0.9322916269302368, 'reward': 0.9322916269302368, 'reward_std': 0.09259742125868797, 'kl': 0.15623219311237335, 'clip_ratio': 0.0, 'epoch': 0.35}\n",
      " 53%|███████████████████▌                 | 106/200 [1:36:31<1:10:09, 44.79s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 106 that is less than the current step 955. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 106 that is less than the current step 955. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_107.txt\n",
      " 54%|███████████████████▊                 | 107/200 [1:37:04<1:04:17, 41.48s/it]INFO:__main__:Logging to wandb at step 107\n",
      "{'loss': -0.0195, 'grad_norm': 0.55078125, 'learning_rate': 3.7337705451608674e-05, 'completion_length': 678.09375, 'rewards/calculate_batch_reward': 0.9375, 'reward': 0.9375, 'reward_std': 0.1111089326441288, 'kl': 0.12958447635173798, 'clip_ratio': 0.0, 'epoch': 0.36}\n",
      " 54%|███████████████████▊                 | 107/200 [1:37:04<1:04:17, 41.48s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 107 that is less than the current step 964. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 107 that is less than the current step 964. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_108.txt\n",
      " 54%|███████████████████▉                 | 108/200 [1:37:40<1:01:10, 39.90s/it]INFO:__main__:Logging to wandb at step 108\n",
      "{'loss': -0.0118, 'grad_norm': 0.58984375, 'learning_rate': 3.6846716561824965e-05, 'completion_length': 701.5, 'rewards/calculate_batch_reward': 0.8958333134651184, 'reward': 0.8958333134651184, 'reward_std': 0.17659492418169975, 'kl': 0.14201277494430542, 'clip_ratio': 0.0, 'epoch': 0.36}\n",
      " 54%|███████████████████▉                 | 108/200 [1:37:40<1:01:10, 39.90s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 108 that is less than the current step 973. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 108 that is less than the current step 973. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_109.txt\n",
      " 55%|████████████████████▏                | 109/200 [1:38:39<1:09:01, 45.51s/it]INFO:__main__:Logging to wandb at step 109\n",
      "{'loss': 0.0687, 'grad_norm': 0.5625, 'learning_rate': 3.634976249348867e-05, 'completion_length': 772.46875, 'rewards/calculate_batch_reward': 0.8854166269302368, 'reward': 0.8854166269302368, 'reward_std': 0.24883989989757538, 'kl': 0.13416313380002975, 'clip_ratio': 0.0, 'epoch': 0.36}\n",
      " 55%|████████████████████▏                | 109/200 [1:38:39<1:09:01, 45.51s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 109 that is less than the current step 982. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 109 that is less than the current step 982. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_110.txt\n",
      " 55%|████████████████████▎                | 110/200 [1:39:17<1:05:05, 43.40s/it]INFO:__main__:Logging to wandb at step 110\n",
      "{'loss': -0.0216, 'grad_norm': 0.52734375, 'learning_rate': 3.5847093477938956e-05, 'completion_length': 688.625, 'rewards/calculate_batch_reward': 0.9427083432674408, 'reward': 0.9427083432674408, 'reward_std': 0.1734592691063881, 'kl': 0.12690073251724243, 'clip_ratio': 0.0, 'epoch': 0.37}\n",
      " 55%|████████████████████▎                | 110/200 [1:39:26<1:05:05, 43.40s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 991. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 110 that is less than the current step 991. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_111.txt\n",
      " 56%|████████████████████▌                | 111/200 [1:40:05<1:06:34, 44.88s/it]INFO:__main__:Logging to wandb at step 111\n",
      "{'loss': 0.0306, 'grad_norm': 0.578125, 'learning_rate': 3.533896262416302e-05, 'completion_length': 744.09375, 'rewards/calculate_batch_reward': 0.9062499701976776, 'reward': 0.9062499701976776, 'reward_std': 0.1111089326441288, 'kl': 0.12842867523431778, 'clip_ratio': 0.0, 'epoch': 0.37}\n",
      " 56%|████████████████████▌                | 111/200 [1:40:05<1:06:34, 44.88s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 111 that is less than the current step 1000. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 111 that is less than the current step 1000. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_112.txt\n",
      " 56%|████████████████████▋                | 112/200 [1:40:41<1:01:34, 41.98s/it]INFO:__main__:Logging to wandb at step 112\n",
      "{'loss': -0.0099, 'grad_norm': 0.60546875, 'learning_rate': 3.4825625791348096e-05, 'completion_length': 715.59375, 'rewards/calculate_batch_reward': 0.9531249701976776, 'reward': 0.9531249701976776, 'reward_std': 0.07353796623647213, 'kl': 0.12455221638083458, 'clip_ratio': 0.0, 'epoch': 0.37}\n",
      " 56%|████████████████████▋                | 112/200 [1:40:42<1:01:34, 41.98s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 112 that is less than the current step 1009. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 112 that is less than the current step 1009. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_113.txt\n",
      " 56%|██████████████████████                 | 113/200 [1:41:17<58:20, 40.24s/it]INFO:__main__:Logging to wandb at step 113\n",
      "{'loss': 0.0014, 'grad_norm': 0.55078125, 'learning_rate': 3.4307341460048633e-05, 'completion_length': 711.65625, 'rewards/calculate_batch_reward': 0.96875, 'reward': 0.96875, 'reward_std': 0.09065093100070953, 'kl': 0.12431599944829941, 'clip_ratio': 0.0, 'epoch': 0.38}\n",
      " 56%|██████████████████████                 | 113/200 [1:41:17<58:20, 40.24s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 113 that is less than the current step 1018. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 113 that is less than the current step 1018. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_114.txt\n",
      " 57%|█████████████████████                | 114/200 [1:42:14<1:04:53, 45.27s/it]INFO:__main__:Logging to wandb at step 114\n",
      "{'loss': 0.083, 'grad_norm': 0.671875, 'learning_rate': 3.378437060203357e-05, 'completion_length': 784.1875, 'rewards/calculate_batch_reward': 0.9062499701976776, 'reward': 0.9062499701976776, 'reward_std': 0.1585075519979, 'kl': 0.12212899699807167, 'clip_ratio': 0.0, 'epoch': 0.38}\n",
      " 57%|█████████████████████                | 114/200 [1:42:14<1:04:53, 45.27s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 114 that is less than the current step 1027. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 114 that is less than the current step 1027. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_115.txt\n",
      " 57%|█████████████████████▎               | 115/200 [1:42:49<1:00:04, 42.41s/it]INFO:__main__:Logging to wandb at step 115\n",
      "{'loss': -0.0019, 'grad_norm': 0.6328125, 'learning_rate': 3.3256976548879184e-05, 'completion_length': 717.53125, 'rewards/calculate_batch_reward': 0.9583333134651184, 'reward': 0.9583333134651184, 'reward_std': 0.07326274551451206, 'kl': 0.12887116521596909, 'clip_ratio': 0.0, 'epoch': 0.38}\n",
      " 57%|█████████████████████▎               | 115/200 [1:42:50<1:00:04, 42.41s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 115 that is less than the current step 1036. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 115 that is less than the current step 1036. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_116.txt\n",
      " 58%|██████████████████████▌                | 116/200 [1:43:29<58:19, 41.66s/it]INFO:__main__:Logging to wandb at step 116\n",
      "{'loss': 0.0068, 'grad_norm': 0.6328125, 'learning_rate': 3.272542485937369e-05, 'completion_length': 747.53125, 'rewards/calculate_batch_reward': 0.8958333134651184, 'reward': 0.8958333134651184, 'reward_std': 0.25, 'kl': 0.12728313356637955, 'clip_ratio': 0.0, 'epoch': 0.39}\n",
      " 58%|██████████████████████▌                | 116/200 [1:43:29<58:19, 41.66s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 116 that is less than the current step 1045. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 116 that is less than the current step 1045. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_117.txt\n",
      " 58%|██████████████████████▊                | 117/200 [1:44:05<55:07, 39.86s/it]INFO:__main__:Logging to wandb at step 117\n",
      "{'loss': 0.0093, 'grad_norm': 0.61328125, 'learning_rate': 3.218998318580043e-05, 'completion_length': 718.875, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.11857912689447403, 'kl': 0.13014310598373413, 'clip_ratio': 0.0, 'epoch': 0.39}\n",
      " 58%|██████████████████████▊                | 117/200 [1:44:05<55:07, 39.86s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 117 that is less than the current step 1054. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 117 that is less than the current step 1054. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_118.txt\n",
      " 59%|███████████████████████                | 118/200 [1:44:43<53:48, 39.37s/it]INFO:__main__:Logging to wandb at step 118\n",
      "{'loss': 0.0123, 'grad_norm': 0.58203125, 'learning_rate': 3.165092113916688e-05, 'completion_length': 736.65625, 'rewards/calculate_batch_reward': 0.9218749701976776, 'reward': 0.9218749701976776, 'reward_std': 0.1269650012254715, 'kl': 0.13594865798950195, 'clip_ratio': 0.0, 'epoch': 0.39}\n",
      " 59%|███████████████████████                | 118/200 [1:44:43<53:48, 39.37s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 118 that is less than the current step 1063. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 118 that is less than the current step 1063. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_119.txt\n",
      " 60%|███████████████████████▏               | 119/200 [1:45:23<53:14, 39.44s/it]INFO:__main__:Logging to wandb at step 119\n",
      "{'loss': 0.004, 'grad_norm': 0.59765625, 'learning_rate': 3.110851015344735e-05, 'completion_length': 756.0625, 'rewards/calculate_batch_reward': 0.9427083134651184, 'reward': 0.9427083134651184, 'reward_std': 0.11735879629850388, 'kl': 0.1277681365609169, 'clip_ratio': 0.0, 'epoch': 0.4}\n",
      " 60%|███████████████████████▏               | 119/200 [1:45:23<53:14, 39.44s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 119 that is less than the current step 1072. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 119 that is less than the current step 1072. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_120.txt\n",
      " 60%|███████████████████████▍               | 120/200 [1:46:01<52:12, 39.15s/it]INFO:__main__:Logging to wandb at step 120\n",
      "{'loss': 0.0356, 'grad_norm': 0.69140625, 'learning_rate': 3.056302334890786e-05, 'completion_length': 745.75, 'rewards/calculate_batch_reward': 0.9010416567325592, 'reward': 0.9010416567325592, 'reward_std': 0.17786166444420815, 'kl': 0.13088976591825485, 'clip_ratio': 0.0, 'epoch': 0.4}\n",
      " 60%|███████████████████████▍               | 120/200 [1:46:01<52:12, 39.15s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 1081. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 120 that is less than the current step 1081. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_121.txt\n",
      " 60%|██████████████████████▍              | 121/200 [1:47:15<1:05:16, 49.57s/it]INFO:__main__:Logging to wandb at step 121\n",
      "{'loss': 0.0561, 'grad_norm': 0.7421875, 'learning_rate': 3.0014735394581823e-05, 'completion_length': 812.90625, 'rewards/calculate_batch_reward': 0.90625, 'reward': 0.90625, 'reward_std': 0.21719226986169815, 'kl': 0.14096377789974213, 'clip_ratio': 0.0, 'epoch': 0.4}\n",
      " 60%|██████████████████████▍              | 121/200 [1:47:15<1:05:16, 49.57s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 121 that is less than the current step 1090. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 121 that is less than the current step 1090. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_122.txt\n",
      " 61%|███████████████████████▊               | 122/200 [1:47:53<59:57, 46.12s/it]INFO:__main__:Logging to wandb at step 122\n",
      "{'loss': -0.0024, 'grad_norm': 0.6640625, 'learning_rate': 2.9463922369965917e-05, 'completion_length': 751.59375, 'rewards/calculate_batch_reward': 0.9322916567325592, 'reward': 0.9322916567325592, 'reward_std': 0.10259523615241051, 'kl': 0.14484840631484985, 'clip_ratio': 0.0, 'epoch': 0.41}\n",
      " 61%|███████████████████████▊               | 122/200 [1:47:53<59:57, 46.12s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 122 that is less than the current step 1099. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 122 that is less than the current step 1099. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_123.txt\n",
      " 62%|███████████████████████▉               | 123/200 [1:48:33<56:52, 44.32s/it]INFO:__main__:Logging to wandb at step 123\n",
      "{'loss': 0.0239, 'grad_norm': 0.65234375, 'learning_rate': 2.8910861626005776e-05, 'completion_length': 749.65625, 'rewards/calculate_batch_reward': 0.8749999701976776, 'reward': 0.8749999701976776, 'reward_std': 0.20153582841157913, 'kl': 0.13666345924139023, 'clip_ratio': 0.0, 'epoch': 0.41}\n",
      " 62%|███████████████████████▉               | 123/200 [1:48:33<56:52, 44.32s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 123 that is less than the current step 1108. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 123 that is less than the current step 1108. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_124.txt\n",
      " 62%|██████████████████████▉              | 124/200 [1:49:50<1:08:18, 53.93s/it]INFO:__main__:Logging to wandb at step 124\n",
      "{'loss': 0.1664, 'grad_norm': 0.80078125, 'learning_rate': 2.8355831645441388e-05, 'completion_length': 857.1875, 'rewards/calculate_batch_reward': 0.84375, 'reward': 0.84375, 'reward_std': 0.2945418655872345, 'kl': 0.12545409053564072, 'clip_ratio': 0.0, 'epoch': 0.41}\n",
      " 62%|██████████████████████▉              | 124/200 [1:49:50<1:08:18, 53.93s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 124 that is less than the current step 1117. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 124 that is less than the current step 1117. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_125.txt\n",
      " 62%|███████████████████████▏             | 125/200 [1:50:51<1:10:04, 56.06s/it]INFO:__main__:Logging to wandb at step 125\n",
      "{'loss': 0.0561, 'grad_norm': 0.625, 'learning_rate': 2.7799111902582696e-05, 'completion_length': 848.59375, 'rewards/calculate_batch_reward': 0.8749999701976776, 'reward': 0.8749999701976776, 'reward_std': 0.22312717139720917, 'kl': 0.14554844796657562, 'clip_ratio': 0.0, 'epoch': 0.42}\n",
      " 62%|███████████████████████▏             | 125/200 [1:50:52<1:10:04, 56.06s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 125 that is less than the current step 1126. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 125 that is less than the current step 1126. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_126.txt\n",
      " 63%|███████████████████████▎             | 126/200 [1:51:48<1:09:44, 56.55s/it]INFO:__main__:Logging to wandb at step 126\n",
      "{'loss': 0.063, 'grad_norm': 0.85546875, 'learning_rate': 2.724098272258584e-05, 'completion_length': 786.03125, 'rewards/calculate_batch_reward': 0.921875, 'reward': 0.921875, 'reward_std': 0.16701342537999153, 'kl': 0.14669091254472733, 'clip_ratio': 0.0, 'epoch': 0.42}\n",
      " 63%|███████████████████████▎             | 126/200 [1:51:50<1:09:44, 56.55s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 126 that is less than the current step 1135. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 126 that is less than the current step 1135. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_127.txt\n",
      " 64%|███████████████████████▍             | 127/200 [1:52:30<1:03:21, 52.07s/it]INFO:__main__:Logging to wandb at step 127\n",
      "{'loss': 0.0212, 'grad_norm': 0.65625, 'learning_rate': 2.6681725140300997e-05, 'completion_length': 757.6875, 'rewards/calculate_batch_reward': 0.9374999701976776, 'reward': 0.9374999701976776, 'reward_std': 0.08997243270277977, 'kl': 0.1585622802376747, 'clip_ratio': 0.0, 'epoch': 0.42}\n",
      " 64%|███████████████████████▍             | 127/200 [1:52:30<1:03:21, 52.07s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 127 that is less than the current step 1144. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 127 that is less than the current step 1144. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_128.txt\n",
      " 64%|████████████████████████▉              | 128/200 [1:53:07<57:03, 47.55s/it]INFO:__main__:Logging to wandb at step 128\n",
      "{'loss': -0.009, 'grad_norm': 0.5546875, 'learning_rate': 2.6121620758762877e-05, 'completion_length': 746.5625, 'rewards/calculate_batch_reward': 0.9270833134651184, 'reward': 0.9270833134651184, 'reward_std': 0.11079318448901176, 'kl': 0.16003672033548355, 'clip_ratio': 0.0, 'epoch': 0.43}\n",
      " 64%|████████████████████████▉              | 128/200 [1:53:07<57:03, 47.55s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 128 that is less than the current step 1153. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 128 that is less than the current step 1153. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_129.txt\n",
      " 64%|█████████████████████████▏             | 129/200 [1:53:49<54:16, 45.87s/it]INFO:__main__:Logging to wandb at step 129\n",
      "{'loss': 0.0539, 'grad_norm': 0.6875, 'learning_rate': 2.556095160739513e-05, 'completion_length': 719.09375, 'rewards/calculate_batch_reward': 0.9010416567325592, 'reward': 0.9010416567325592, 'reward_std': 0.17972318828105927, 'kl': 0.16954530775547028, 'clip_ratio': 0.0, 'epoch': 0.43}\n",
      " 64%|█████████████████████████▏             | 129/200 [1:53:49<54:16, 45.87s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 129 that is less than the current step 1162. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 129 that is less than the current step 1162. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_130.txt\n",
      " 65%|█████████████████████████▎             | 130/200 [1:54:23<49:16, 42.23s/it]INFO:__main__:Logging to wandb at step 130\n",
      "{'loss': 0.0135, 'grad_norm': 0.578125, 'learning_rate': 2.5e-05, 'completion_length': 720.625, 'rewards/calculate_batch_reward': 0.8958333432674408, 'reward': 0.8958333432674408, 'reward_std': 0.18688659742474556, 'kl': 0.15673290193080902, 'clip_ratio': 0.0, 'epoch': 0.43}\n",
      " 65%|█████████████████████████▎             | 130/200 [1:54:23<49:16, 42.23s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 1171. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 130 that is less than the current step 1171. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_131.txt\n",
      " 66%|█████████████████████████▌             | 131/200 [1:54:58<46:12, 40.19s/it]INFO:__main__:Logging to wandb at step 131\n",
      "{'loss': 0.0286, 'grad_norm': 0.69921875, 'learning_rate': 2.443904839260488e-05, 'completion_length': 698.28125, 'rewards/calculate_batch_reward': 0.953125, 'reward': 0.953125, 'reward_std': 0.08518768101930618, 'kl': 0.16294661909341812, 'clip_ratio': 0.0, 'epoch': 0.44}\n",
      " 66%|█████████████████████████▌             | 131/200 [1:54:58<46:12, 40.19s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 131 that is less than the current step 1180. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 131 that is less than the current step 1180. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_132.txt\n",
      " 66%|█████████████████████████▋             | 132/200 [1:55:32<43:24, 38.31s/it]INFO:__main__:Logging to wandb at step 132\n",
      "{'loss': 0.0413, 'grad_norm': 0.84375, 'learning_rate': 2.3878379241237136e-05, 'completion_length': 685.34375, 'rewards/calculate_batch_reward': 0.9270833134651184, 'reward': 0.9270833134651184, 'reward_std': 0.19921721518039703, 'kl': 0.15193235874176025, 'clip_ratio': 0.0, 'epoch': 0.44}\n",
      " 66%|█████████████████████████▋             | 132/200 [1:55:32<43:24, 38.31s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 132 that is less than the current step 1189. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 132 that is less than the current step 1189. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_133.txt\n",
      " 66%|█████████████████████████▉             | 133/200 [1:56:04<40:29, 36.26s/it]INFO:__main__:Logging to wandb at step 133\n",
      "{'loss': -0.0253, 'grad_norm': 0.65234375, 'learning_rate': 2.331827485969901e-05, 'completion_length': 673.65625, 'rewards/calculate_batch_reward': 0.9583333134651184, 'reward': 0.9583333134651184, 'reward_std': 0.07348554208874702, 'kl': 0.16149432212114334, 'clip_ratio': 0.0, 'epoch': 0.44}\n",
      " 66%|█████████████████████████▉             | 133/200 [1:56:07<40:29, 36.26s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 133 that is less than the current step 1198. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 133 that is less than the current step 1198. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_134.txt\n",
      " 67%|██████████████████████████▏            | 134/200 [1:56:38<39:10, 35.61s/it]INFO:__main__:Logging to wandb at step 134\n",
      "{'loss': 0.0536, 'grad_norm': 0.89453125, 'learning_rate': 2.2759017277414166e-05, 'completion_length': 680.84375, 'rewards/calculate_batch_reward': 0.8645833134651184, 'reward': 0.8645833134651184, 'reward_std': 0.18410607427358627, 'kl': 0.21826480329036713, 'clip_ratio': 0.0, 'epoch': 0.45}\n",
      " 67%|██████████████████████████▏            | 134/200 [1:56:38<39:10, 35.61s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 134 that is less than the current step 1207. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 134 that is less than the current step 1207. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_135.txt\n",
      " 68%|██████████████████████████▎            | 135/200 [1:57:13<38:36, 35.64s/it]INFO:__main__:Logging to wandb at step 135\n",
      "{'loss': 0.0333, 'grad_norm': 0.79296875, 'learning_rate': 2.2200888097417307e-05, 'completion_length': 712.21875, 'rewards/calculate_batch_reward': 0.9166666567325592, 'reward': 0.9166666567325592, 'reward_std': 0.17659493535757065, 'kl': 0.18977399915456772, 'clip_ratio': 0.0, 'epoch': 0.45}\n",
      " 68%|██████████████████████████▎            | 135/200 [1:57:13<38:36, 35.64s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 135 that is less than the current step 1216. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 135 that is less than the current step 1216. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_136.txt\n",
      " 68%|██████████████████████████▌            | 136/200 [1:58:33<52:04, 48.82s/it]INFO:__main__:Logging to wandb at step 136\n",
      "{'loss': 0.0109, 'grad_norm': 0.65234375, 'learning_rate': 2.164416835455862e-05, 'completion_length': 700.34375, 'rewards/calculate_batch_reward': 0.9062499701976776, 'reward': 0.9062499701976776, 'reward_std': 0.1778181865811348, 'kl': 0.18312640488147736, 'clip_ratio': 0.0, 'epoch': 0.45}\n",
      " 68%|██████████████████████████▌            | 136/200 [1:58:33<52:04, 48.82s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 136 that is less than the current step 1225. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 136 that is less than the current step 1225. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_137.txt\n",
      " 68%|██████████████████████████▋            | 137/200 [1:59:09<47:14, 44.99s/it]INFO:__main__:Logging to wandb at step 137\n",
      "{'loss': 0.0322, 'grad_norm': 0.734375, 'learning_rate': 2.1089138373994223e-05, 'completion_length': 667.28125, 'rewards/calculate_batch_reward': 0.9375, 'reward': 0.9375, 'reward_std': 0.10260272026062012, 'kl': 0.1907566487789154, 'clip_ratio': 0.0, 'epoch': 0.46}\n",
      " 68%|██████████████████████████▋            | 137/200 [1:59:09<47:14, 44.99s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 137 that is less than the current step 1234. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 137 that is less than the current step 1234. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_138.txt\n",
      " 69%|██████████████████████████▉            | 138/200 [1:59:47<44:15, 42.83s/it]INFO:__main__:Logging to wandb at step 138\n",
      "{'loss': 0.0068, 'grad_norm': 0.6484375, 'learning_rate': 2.0536077630034086e-05, 'completion_length': 692.3125, 'rewards/calculate_batch_reward': 0.8541666269302368, 'reward': 0.8541666269302368, 'reward_std': 0.29238365590572357, 'kl': 0.18169313669204712, 'clip_ratio': 0.0, 'epoch': 0.46}\n",
      " 69%|██████████████████████████▉            | 138/200 [1:59:47<44:15, 42.83s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 138 that is less than the current step 1243. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 138 that is less than the current step 1243. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_139.txt\n",
      " 70%|███████████████████████████            | 139/200 [2:00:20<40:34, 39.91s/it]INFO:__main__:Logging to wandb at step 139\n",
      "{'loss': 0.0204, 'grad_norm': 0.6171875, 'learning_rate': 1.9985264605418183e-05, 'completion_length': 650.75, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.16718658432364464, 'kl': 0.18080618232488632, 'clip_ratio': 0.0, 'epoch': 0.46}\n",
      " 70%|███████████████████████████            | 139/200 [2:00:20<40:34, 39.91s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 139 that is less than the current step 1252. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 139 that is less than the current step 1252. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_140.txt\n",
      " 70%|███████████████████████████▎           | 140/200 [2:00:53<37:54, 37.90s/it]INFO:__main__:Logging to wandb at step 140\n",
      "{'loss': -0.0118, 'grad_norm': 0.8046875, 'learning_rate': 1.9436976651092144e-05, 'completion_length': 661.125, 'rewards/calculate_batch_reward': 0.9218749701976776, 'reward': 0.9218749701976776, 'reward_std': 0.25080884993076324, 'kl': 0.194375179708004, 'clip_ratio': 0.0, 'epoch': 0.47}\n",
      " 70%|███████████████████████████▎           | 140/200 [2:00:53<37:54, 37.90s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140 that is less than the current step 1261. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 140 that is less than the current step 1261. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_141.txt\n",
      " 70%|███████████████████████████▍           | 141/200 [2:01:32<37:40, 38.32s/it]INFO:__main__:Logging to wandb at step 141\n",
      "{'loss': 0.0403, 'grad_norm': 0.73046875, 'learning_rate': 1.8891489846552646e-05, 'completion_length': 656.78125, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.1713460311293602, 'kl': 0.17724819481372833, 'clip_ratio': 0.0, 'epoch': 0.47}\n",
      " 70%|███████████████████████████▍           | 141/200 [2:01:45<37:40, 38.32s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 141 that is less than the current step 1270. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 141 that is less than the current step 1270. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_142.txt\n",
      " 71%|███████████████████████████▋           | 142/200 [2:02:20<39:36, 40.97s/it]INFO:__main__:Logging to wandb at step 142\n",
      "{'loss': 0.0432, 'grad_norm': 0.83203125, 'learning_rate': 1.8349078860833123e-05, 'completion_length': 685.59375, 'rewards/calculate_batch_reward': 0.7760416567325592, 'reward': 0.7760416567325592, 'reward_std': 0.39249347150325775, 'kl': 0.19408035278320312, 'clip_ratio': 0.0, 'epoch': 0.47}\n",
      " 71%|███████████████████████████▋           | 142/200 [2:02:20<39:36, 40.97s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 142 that is less than the current step 1279. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 142 that is less than the current step 1279. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_143.txt\n",
      " 72%|███████████████████████████▉           | 143/200 [2:03:02<39:17, 41.36s/it]INFO:__main__:Logging to wandb at step 143\n",
      "{'loss': -0.0141, 'grad_norm': 0.93359375, 'learning_rate': 1.781001681419957e-05, 'completion_length': 654.15625, 'rewards/calculate_batch_reward': 0.8854166269302368, 'reward': 0.8854166269302368, 'reward_std': 0.2504621148109436, 'kl': 0.22059427201747894, 'clip_ratio': 0.0, 'epoch': 0.48}\n",
      " 72%|███████████████████████████▉           | 143/200 [2:03:02<39:17, 41.36s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 143 that is less than the current step 1288. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 143 that is less than the current step 1288. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_144.txt\n",
      " 72%|████████████████████████████           | 144/200 [2:03:32<35:20, 37.87s/it]INFO:__main__:Logging to wandb at step 144\n",
      "{'loss': 0.0267, 'grad_norm': 0.80078125, 'learning_rate': 1.7274575140626318e-05, 'completion_length': 630.4375, 'rewards/calculate_batch_reward': 0.828125, 'reward': 0.828125, 'reward_std': 0.3201445862650871, 'kl': 0.2220321148633957, 'clip_ratio': 0.0, 'epoch': 0.48}\n",
      " 72%|████████████████████████████           | 144/200 [2:03:32<35:20, 37.87s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 144 that is less than the current step 1297. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 144 that is less than the current step 1297. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_145.txt\n",
      " 72%|████████████████████████████▎          | 145/200 [2:04:06<33:50, 36.91s/it]INFO:__main__:Logging to wandb at step 145\n",
      "{'loss': 0.0221, 'grad_norm': 1.0859375, 'learning_rate': 1.6743023451120832e-05, 'completion_length': 625.96875, 'rewards/calculate_batch_reward': 0.8072916269302368, 'reward': 0.8072916269302368, 'reward_std': 0.3638463467359543, 'kl': 0.21013512462377548, 'clip_ratio': 0.0, 'epoch': 0.48}\n",
      " 72%|████████████████████████████▎          | 145/200 [2:04:06<33:50, 36.91s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 145 that is less than the current step 1306. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 145 that is less than the current step 1306. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_146.txt\n",
      " 73%|████████████████████████████▍          | 146/200 [2:04:56<36:35, 40.66s/it]INFO:__main__:Logging to wandb at step 146\n",
      "{'loss': 0.0681, 'grad_norm': 1.109375, 'learning_rate': 1.621562939796643e-05, 'completion_length': 661.46875, 'rewards/calculate_batch_reward': 0.8229166567325592, 'reward': 0.8229166567325592, 'reward_std': 0.33017846941947937, 'kl': 0.19808955490589142, 'clip_ratio': 0.0, 'epoch': 0.49}\n",
      " 73%|████████████████████████████▍          | 146/200 [2:04:56<36:35, 40.66s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 146 that is less than the current step 1315. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 146 that is less than the current step 1315. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_147.txt\n",
      " 74%|████████████████████████████▋          | 147/200 [2:05:32<34:41, 39.28s/it]INFO:__main__:Logging to wandb at step 147\n",
      "{'loss': 0.0185, 'grad_norm': 0.859375, 'learning_rate': 1.5692658539951372e-05, 'completion_length': 658.78125, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.16714933887124062, 'kl': 0.2535805404186249, 'clip_ratio': 0.0, 'epoch': 0.49}\n",
      " 74%|████████████████████████████▋          | 147/200 [2:05:32<34:41, 39.28s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 147 that is less than the current step 1324. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 147 that is less than the current step 1324. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_148.txt\n",
      " 74%|████████████████████████████▊          | 148/200 [2:06:07<32:57, 38.03s/it]INFO:__main__:Logging to wandb at step 148\n",
      "{'loss': -0.0057, 'grad_norm': 0.609375, 'learning_rate': 1.5174374208651912e-05, 'completion_length': 625.46875, 'rewards/calculate_batch_reward': 0.9062499701976776, 'reward': 0.9062499701976776, 'reward_std': 0.1764400452375412, 'kl': 0.21807648241519928, 'clip_ratio': 0.0, 'epoch': 0.49}\n",
      " 74%|████████████████████████████▊          | 148/200 [2:06:07<32:57, 38.03s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 148 that is less than the current step 1333. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 148 that is less than the current step 1333. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_149.txt\n",
      " 74%|█████████████████████████████          | 149/200 [2:06:49<33:21, 39.25s/it]INFO:__main__:Logging to wandb at step 149\n",
      "{'loss': 0.036, 'grad_norm': 0.828125, 'learning_rate': 1.466103737583699e-05, 'completion_length': 676.71875, 'rewards/calculate_batch_reward': 0.8749999701976776, 'reward': 0.8749999701976776, 'reward_std': 0.29443834722042084, 'kl': 0.24356430023908615, 'clip_ratio': 0.0, 'epoch': 0.5}\n",
      " 74%|█████████████████████████████          | 149/200 [2:06:49<33:21, 39.25s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 149 that is less than the current step 1342. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 149 that is less than the current step 1342. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_150.txt\n",
      " 75%|█████████████████████████████▎         | 150/200 [2:07:20<30:34, 36.69s/it]INFO:__main__:Logging to wandb at step 150\n",
      "{'loss': 0.0046, 'grad_norm': 0.60546875, 'learning_rate': 1.4152906522061048e-05, 'completion_length': 671.4375, 'rewards/calculate_batch_reward': 0.9375, 'reward': 0.9375, 'reward_std': 0.12729376554489136, 'kl': 0.2516763284802437, 'clip_ratio': 0.0, 'epoch': 0.5}\n",
      " 75%|█████████████████████████████▎         | 150/200 [2:07:20<30:34, 36.69s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 1351. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 150 that is less than the current step 1351. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_151.txt\n",
      " 76%|█████████████████████████████▍         | 151/200 [2:08:43<41:24, 50.70s/it]INFO:__main__:Logging to wandb at step 151\n",
      "{'loss': 0.0253, 'grad_norm': 0.75390625, 'learning_rate': 1.3650237506511331e-05, 'completion_length': 636.5, 'rewards/calculate_batch_reward': 0.8072916567325592, 'reward': 0.8072916567325592, 'reward_std': 0.26778270304203033, 'kl': 0.25975483655929565, 'clip_ratio': 0.0, 'epoch': 0.5}\n",
      " 76%|█████████████████████████████▍         | 151/200 [2:08:43<41:24, 50.70s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 151 that is less than the current step 1360. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 151 that is less than the current step 1360. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_152.txt\n",
      " 76%|█████████████████████████████▋         | 152/200 [2:09:16<36:12, 45.25s/it]INFO:__main__:Logging to wandb at step 152\n",
      "{'loss': 0.045, 'grad_norm': 0.9140625, 'learning_rate': 1.3153283438175034e-05, 'completion_length': 622.1875, 'rewards/calculate_batch_reward': 0.9270833730697632, 'reward': 0.9270833730697632, 'reward_std': 0.11304926127195358, 'kl': 0.2683105394244194, 'clip_ratio': 0.0, 'epoch': 0.51}\n",
      " 76%|█████████████████████████████▋         | 152/200 [2:09:16<36:12, 45.25s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 152 that is less than the current step 1369. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 152 that is less than the current step 1369. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_153.txt\n",
      " 76%|█████████████████████████████▊         | 153/200 [2:09:49<32:45, 41.82s/it]INFO:__main__:Logging to wandb at step 153\n",
      "{'loss': 0.016, 'grad_norm': 0.65625, 'learning_rate': 1.2662294548391328e-05, 'completion_length': 634.4375, 'rewards/calculate_batch_reward': 0.8124999701976776, 'reward': 0.8124999701976776, 'reward_std': 0.3650396913290024, 'kl': 0.17405719310045242, 'clip_ratio': 0.0, 'epoch': 0.51}\n",
      " 76%|█████████████████████████████▊         | 153/200 [2:09:50<32:45, 41.82s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 153 that is less than the current step 1378. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 153 that is less than the current step 1378. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_154.txt\n",
      " 77%|██████████████████████████████         | 154/200 [2:10:40<34:07, 44.51s/it]INFO:__main__:Logging to wandb at step 154\n",
      "{'loss': 0.0671, 'grad_norm': 1.25, 'learning_rate': 1.217751806485235e-05, 'completion_length': 666.375, 'rewards/calculate_batch_reward': 0.8489583134651184, 'reward': 0.8489583134651184, 'reward_std': 0.257399320602417, 'kl': 0.22527199238538742, 'clip_ratio': 0.0, 'epoch': 0.51}\n",
      " 77%|██████████████████████████████         | 154/200 [2:10:40<34:07, 44.51s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 154 that is less than the current step 1387. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 154 that is less than the current step 1387. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_155.txt\n",
      " 78%|██████████████████████████████▏        | 155/200 [2:11:11<30:18, 40.42s/it]INFO:__main__:Logging to wandb at step 155\n",
      "{'loss': 0.023, 'grad_norm': 0.73828125, 'learning_rate': 1.1699198087116589e-05, 'completion_length': 649.65625, 'rewards/calculate_batch_reward': 0.9479166567325592, 'reward': 0.9479166567325592, 'reward_std': 0.0997074581682682, 'kl': 0.24940955638885498, 'clip_ratio': 0.0, 'epoch': 0.52}\n",
      " 78%|██████████████████████████████▏        | 155/200 [2:11:11<30:18, 40.42s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 155 that is less than the current step 1396. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 155 that is less than the current step 1396. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_156.txt\n",
      " 78%|██████████████████████████████▍        | 156/200 [2:12:19<35:43, 48.72s/it]INFO:__main__:Logging to wandb at step 156\n",
      "{'loss': 0.0111, 'grad_norm': 1.0390625, 'learning_rate': 1.122757546369744e-05, 'completion_length': 694.34375, 'rewards/calculate_batch_reward': 0.7968749701976776, 'reward': 0.7968749701976776, 'reward_std': 0.3448992297053337, 'kl': 0.3320121765136719, 'clip_ratio': 0.0, 'epoch': 0.52}\n",
      " 78%|██████████████████████████████▍        | 156/200 [2:12:19<35:43, 48.72s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 156 that is less than the current step 1405. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 156 that is less than the current step 1405. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_157.txt\n",
      " 78%|██████████████████████████████▌        | 157/200 [2:12:49<30:51, 43.05s/it]INFO:__main__:Logging to wandb at step 157\n",
      "{'loss': 0.0079, 'grad_norm': 0.80859375, 'learning_rate': 1.0762887670788702e-05, 'completion_length': 658.28125, 'rewards/calculate_batch_reward': 0.9479166269302368, 'reward': 0.9479166269302368, 'reward_std': 0.15392587520182133, 'kl': 0.24482975155115128, 'clip_ratio': 0.0, 'epoch': 0.52}\n",
      " 78%|██████████████████████████████▌        | 157/200 [2:12:49<30:51, 43.05s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 157 that is less than the current step 1414. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 157 that is less than the current step 1414. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_158.txt\n",
      " 79%|██████████████████████████████▊        | 158/200 [2:13:22<28:05, 40.12s/it]INFO:__main__:Logging to wandb at step 158\n",
      "{'loss': 0.0522, 'grad_norm': 1.71875, 'learning_rate': 1.0305368692688174e-05, 'completion_length': 624.8125, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.1832466498017311, 'kl': 0.2130577191710472, 'clip_ratio': 0.0, 'epoch': 0.53}\n",
      " 79%|██████████████████████████████▊        | 158/200 [2:13:22<28:05, 40.12s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 158 that is less than the current step 1423. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 158 that is less than the current step 1423. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_159.txt\n",
      " 80%|███████████████████████████████        | 159/200 [2:14:12<29:27, 43.11s/it]INFO:__main__:Logging to wandb at step 159\n",
      "{'loss': 0.0346, 'grad_norm': 0.85546875, 'learning_rate': 9.855248903979506e-06, 'completion_length': 657.09375, 'rewards/calculate_batch_reward': 0.90625, 'reward': 0.90625, 'reward_std': 0.25433406233787537, 'kl': 0.2555966079235077, 'clip_ratio': 0.0, 'epoch': 0.53}\n",
      " 80%|███████████████████████████████        | 159/200 [2:14:12<29:27, 43.11s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 159 that is less than the current step 1432. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 159 that is less than the current step 1432. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_160.txt\n",
      " 80%|███████████████████████████████▏       | 160/200 [2:14:42<26:03, 39.08s/it]INFO:__main__:Logging to wandb at step 160\n",
      "{'loss': 0.0106, 'grad_norm': 0.77734375, 'learning_rate': 9.412754953531663e-06, 'completion_length': 605.90625, 'rewards/calculate_batch_reward': 0.8333333134651184, 'reward': 0.8333333134651184, 'reward_std': 0.33592739701271057, 'kl': 0.2371317371726036, 'clip_ratio': 0.0, 'epoch': 0.53}\n",
      " 80%|███████████████████████████████▏       | 160/200 [2:14:42<26:03, 39.08s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 160 that is less than the current step 1441. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 160 that is less than the current step 1441. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_161.txt\n",
      " 80%|███████████████████████████████▍       | 161/200 [2:15:13<23:44, 36.53s/it]INFO:__main__:Logging to wandb at step 161\n",
      "{'loss': 0.0266, 'grad_norm': 0.87109375, 'learning_rate': 8.978109650374397e-06, 'completion_length': 624.34375, 'rewards/calculate_batch_reward': 0.8177083134651184, 'reward': 0.8177083134651184, 'reward_std': 0.36633671820163727, 'kl': 0.2951366752386093, 'clip_ratio': 0.0, 'epoch': 0.54}\n",
      " 80%|███████████████████████████████▍       | 161/200 [2:15:13<23:44, 36.53s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 161 that is less than the current step 1450. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 161 that is less than the current step 1450. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_162.txt\n",
      " 81%|███████████████████████████████▌       | 162/200 [2:15:54<24:01, 37.94s/it]INFO:__main__:Logging to wandb at step 162\n",
      "{'loss': 0.0335, 'grad_norm': 0.81640625, 'learning_rate': 8.551531851507186e-06, 'completion_length': 665.375, 'rewards/calculate_batch_reward': 0.8385416567325592, 'reward': 0.8385416567325592, 'reward_std': 0.3278231620788574, 'kl': 0.3103928416967392, 'clip_ratio': 0.0, 'epoch': 0.54}\n",
      " 81%|███████████████████████████████▌       | 162/200 [2:15:54<24:01, 37.94s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 162 that is less than the current step 1459. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 162 that is less than the current step 1459. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_163.txt\n",
      " 82%|███████████████████████████████▊       | 163/200 [2:16:25<22:11, 35.99s/it]INFO:__main__:Logging to wandb at step 163\n",
      "{'loss': -0.0247, 'grad_norm': 0.78125, 'learning_rate': 8.133236351698143e-06, 'completion_length': 608.0625, 'rewards/calculate_batch_reward': 0.7760416567325592, 'reward': 0.7760416567325592, 'reward_std': 0.30187394097447395, 'kl': 0.26698528975248337, 'clip_ratio': 0.0, 'epoch': 0.54}\n",
      " 82%|███████████████████████████████▊       | 163/200 [2:16:25<22:11, 35.99s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 163 that is less than the current step 1468. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 163 that is less than the current step 1468. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_164.txt\n",
      " 82%|███████████████████████████████▉       | 164/200 [2:17:02<21:42, 36.17s/it]INFO:__main__:Logging to wandb at step 164\n",
      "{'loss': 0.0247, 'grad_norm': 1.2578125, 'learning_rate': 7.723433775328384e-06, 'completion_length': 645.375, 'rewards/calculate_batch_reward': 0.9010416567325592, 'reward': 0.9010416567325592, 'reward_std': 0.20025941729545593, 'kl': 0.2598220854997635, 'clip_ratio': 0.0, 'epoch': 0.55}\n",
      " 82%|███████████████████████████████▉       | 164/200 [2:17:06<21:42, 36.17s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 164 that is less than the current step 1477. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 164 that is less than the current step 1477. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_165.txt\n",
      " 82%|████████████████████████████████▏      | 165/200 [2:17:39<21:14, 36.41s/it]INFO:__main__:Logging to wandb at step 165\n",
      "{'loss': -0.0096, 'grad_norm': 0.70703125, 'learning_rate': 7.3223304703363135e-06, 'completion_length': 632.5, 'rewards/calculate_batch_reward': 0.8802083134651184, 'reward': 0.8802083134651184, 'reward_std': 0.2250050399452448, 'kl': 0.24584128707647324, 'clip_ratio': 0.0, 'epoch': 0.55}\n",
      " 82%|████████████████████████████████▏      | 165/200 [2:17:39<21:14, 36.41s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 165 that is less than the current step 1486. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 165 that is less than the current step 1486. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_166.txt\n",
      " 83%|████████████████████████████████▎      | 166/200 [2:19:12<30:15, 53.41s/it]INFO:__main__:Logging to wandb at step 166\n",
      "{'loss': 0.0538, 'grad_norm': 1.0546875, 'learning_rate': 6.930128404315214e-06, 'completion_length': 664.5, 'rewards/calculate_batch_reward': 0.9427083134651184, 'reward': 0.9427083134651184, 'reward_std': 0.08908362500369549, 'kl': 0.25390076637268066, 'clip_ratio': 0.0, 'epoch': 0.55}\n",
      " 83%|████████████████████████████████▎      | 166/200 [2:19:12<30:15, 53.41s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 166 that is less than the current step 1495. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 166 that is less than the current step 1495. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_167.txt\n",
      " 84%|████████████████████████████████▌      | 167/200 [2:20:06<29:25, 53.51s/it]INFO:__main__:Logging to wandb at step 167\n",
      "{'loss': 0.076, 'grad_norm': 1.6796875, 'learning_rate': 6.547025062816486e-06, 'completion_length': 700.46875, 'rewards/calculate_batch_reward': 0.8385416567325592, 'reward': 0.8385416567325592, 'reward_std': 0.3251916766166687, 'kl': 0.374968022108078, 'clip_ratio': 0.0, 'epoch': 0.56}\n",
      " 84%|████████████████████████████████▌      | 167/200 [2:20:06<29:25, 53.51s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 167 that is less than the current step 1504. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 167 that is less than the current step 1504. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_168.txt\n",
      " 84%|████████████████████████████████▊      | 168/200 [2:20:42<25:42, 48.21s/it]INFO:__main__:Logging to wandb at step 168\n",
      "{'loss': 0.0268, 'grad_norm': 0.70703125, 'learning_rate': 6.173213349909729e-06, 'completion_length': 638.125, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.18107980117201805, 'kl': 0.21120639890432358, 'clip_ratio': 0.0, 'epoch': 0.56}\n",
      " 84%|████████████████████████████████▊      | 168/200 [2:20:42<25:42, 48.21s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 168 that is less than the current step 1513. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 168 that is less than the current step 1513. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_169.txt\n",
      " 84%|████████████████████████████████▉      | 169/200 [2:21:14<22:30, 43.55s/it]INFO:__main__:Logging to wandb at step 169\n",
      "{'loss': -0.015, 'grad_norm': 0.734375, 'learning_rate': 5.808881491049723e-06, 'completion_length': 605.46875, 'rewards/calculate_batch_reward': 0.8802083134651184, 'reward': 0.8802083134651184, 'reward_std': 0.29442648589611053, 'kl': 0.19794126600027084, 'clip_ratio': 0.0, 'epoch': 0.56}\n",
      " 84%|████████████████████████████████▉      | 169/200 [2:21:14<22:30, 43.55s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 169 that is less than the current step 1522. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 169 that is less than the current step 1522. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_170.txt\n",
      " 85%|█████████████████████████████████▏     | 170/200 [2:21:47<20:07, 40.25s/it]INFO:__main__:Logging to wandb at step 170\n",
      "{'loss': -0.0427, 'grad_norm': 0.83203125, 'learning_rate': 5.454212938299255e-06, 'completion_length': 628.75, 'rewards/calculate_batch_reward': 0.921875, 'reward': 0.921875, 'reward_std': 0.17694168537855148, 'kl': 0.265794575214386, 'clip_ratio': 0.0, 'epoch': 0.57}\n",
      " 85%|█████████████████████████████████▏     | 170/200 [2:21:47<20:07, 40.25s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 170 that is less than the current step 1531. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 170 that is less than the current step 1531. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_171.txt\n",
      " 86%|█████████████████████████████████▎     | 171/200 [2:22:18<18:07, 37.50s/it]INFO:__main__:Logging to wandb at step 171\n",
      "{'loss': 0.0147, 'grad_norm': 0.84765625, 'learning_rate': 5.1093862779554776e-06, 'completion_length': 657.1875, 'rewards/calculate_batch_reward': 0.8177083134651184, 'reward': 0.8177083134651184, 'reward_std': 0.3501117676496506, 'kl': 0.26534587144851685, 'clip_ratio': 0.0, 'epoch': 0.57}\n",
      " 86%|█████████████████████████████████▎     | 171/200 [2:22:21<18:07, 37.50s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 171 that is less than the current step 1540. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 171 that is less than the current step 1540. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_172.txt\n",
      " 86%|█████████████████████████████████▌     | 172/200 [2:22:51<16:56, 36.31s/it]INFO:__main__:Logging to wandb at step 172\n",
      "{'loss': -0.0174, 'grad_norm': 1.015625, 'learning_rate': 4.7745751406263165e-06, 'completion_length': 655.21875, 'rewards/calculate_batch_reward': 0.828125, 'reward': 0.828125, 'reward_std': 0.3238350450992584, 'kl': 0.3132430911064148, 'clip_ratio': 0.0, 'epoch': 0.57}\n",
      " 86%|█████████████████████████████████▌     | 172/200 [2:22:51<16:56, 36.31s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 172 that is less than the current step 1549. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 172 that is less than the current step 1549. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_173.txt\n",
      " 86%|█████████████████████████████████▋     | 173/200 [2:23:23<15:45, 35.03s/it]INFO:__main__:Logging to wandb at step 173\n",
      "{'loss': -0.0078, 'grad_norm': 1.03125, 'learning_rate': 4.4499481138022544e-06, 'completion_length': 634.8125, 'rewards/calculate_batch_reward': 0.8437499701976776, 'reward': 0.8437499701976776, 'reward_std': 0.29050904512405396, 'kl': 0.2800769880414009, 'clip_ratio': 0.0, 'epoch': 0.58}\n",
      " 86%|█████████████████████████████████▋     | 173/200 [2:23:23<15:45, 35.03s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 173 that is less than the current step 1558. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 173 that is less than the current step 1558. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_174.txt\n",
      " 87%|█████████████████████████████████▉     | 174/200 [2:23:58<15:07, 34.90s/it]INFO:__main__:Logging to wandb at step 174\n",
      "{'loss': 0.0049, 'grad_norm': 0.73046875, 'learning_rate': 4.135668656967434e-06, 'completion_length': 659.59375, 'rewards/calculate_batch_reward': 0.9322916269302368, 'reward': 0.9322916269302368, 'reward_std': 0.15655064024031162, 'kl': 0.28002506494522095, 'clip_ratio': 0.0, 'epoch': 0.58}\n",
      " 87%|█████████████████████████████████▉     | 174/200 [2:23:58<15:07, 34.90s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 174 that is less than the current step 1567. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 174 that is less than the current step 1567. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_175.txt\n",
      " 88%|██████████████████████████████████▏    | 175/200 [2:24:50<16:43, 40.15s/it]INFO:__main__:Logging to wandb at step 175\n",
      "{'loss': 0.067, 'grad_norm': 0.953125, 'learning_rate': 3.831895019292897e-06, 'completion_length': 670.25, 'rewards/calculate_batch_reward': 0.8489583134651184, 'reward': 0.8489583134651184, 'reward_std': 0.291967011988163, 'kl': 0.26368722319602966, 'clip_ratio': 0.0, 'epoch': 0.58}\n",
      " 88%|██████████████████████████████████▏    | 175/200 [2:25:03<16:43, 40.15s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 175 that is less than the current step 1576. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 175 that is less than the current step 1576. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_176.txt\n",
      " 88%|██████████████████████████████████▎    | 176/200 [2:25:33<16:19, 40.82s/it]INFO:__main__:Logging to wandb at step 176\n",
      "{'loss': -0.0273, 'grad_norm': 0.91015625, 'learning_rate': 3.5387801599533475e-06, 'completion_length': 605.375, 'rewards/calculate_batch_reward': 0.8854166567325592, 'reward': 0.8854166567325592, 'reward_std': 0.1788887083530426, 'kl': 0.22681649029254913, 'clip_ratio': 0.0, 'epoch': 0.59}\n",
      " 88%|██████████████████████████████████▎    | 176/200 [2:25:45<16:19, 40.82s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 176 that is less than the current step 1585. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 176 that is less than the current step 1585. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_177.txt\n",
      " 88%|██████████████████████████████████▌    | 177/200 [2:26:16<15:56, 41.57s/it]INFO:__main__:Logging to wandb at step 177\n",
      "{'loss': -0.0004, 'grad_norm': 0.77734375, 'learning_rate': 3.2564716711076167e-06, 'completion_length': 617.0, 'rewards/calculate_batch_reward': 0.8958333134651184, 'reward': 0.8958333134651184, 'reward_std': 0.20841462165117264, 'kl': 0.2362244352698326, 'clip_ratio': 0.0, 'epoch': 0.59}\n",
      " 88%|██████████████████████████████████▌    | 177/200 [2:26:29<15:56, 41.57s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 177 that is less than the current step 1594. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 177 that is less than the current step 1594. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_178.txt\n",
      " 89%|██████████████████████████████████▋    | 178/200 [2:27:00<15:31, 42.32s/it]INFO:__main__:Logging to wandb at step 178\n",
      "{'loss': 0.0394, 'grad_norm': 1.0, 'learning_rate': 2.98511170358155e-06, 'completion_length': 619.5, 'rewards/calculate_batch_reward': 0.890625, 'reward': 0.890625, 'reward_std': 0.21955863013863564, 'kl': 0.2203557938337326, 'clip_ratio': 0.0, 'epoch': 0.59}\n",
      " 89%|██████████████████████████████████▋    | 178/200 [2:27:00<15:31, 42.32s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 178 that is less than the current step 1603. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 178 that is less than the current step 1603. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_179.txt\n",
      " 90%|██████████████████████████████████▉    | 179/200 [2:27:30<13:27, 38.47s/it]INFO:__main__:Logging to wandb at step 179\n",
      "{'loss': 0.0131, 'grad_norm': 0.81640625, 'learning_rate': 2.7248368952908053e-06, 'completion_length': 632.21875, 'rewards/calculate_batch_reward': 0.9010416269302368, 'reward': 0.9010416269302368, 'reward_std': 0.1852780096232891, 'kl': 0.31151433289051056, 'clip_ratio': 0.0, 'epoch': 0.6}\n",
      " 90%|██████████████████████████████████▉    | 179/200 [2:27:30<13:27, 38.47s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 179 that is less than the current step 1612. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 179 that is less than the current step 1612. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_180.txt\n",
      " 90%|███████████████████████████████████    | 180/200 [2:28:23<14:17, 42.85s/it]INFO:__main__:Logging to wandb at step 180\n",
      "{'loss': -0.05, 'grad_norm': 0.79296875, 'learning_rate': 2.475778302439524e-06, 'completion_length': 642.4375, 'rewards/calculate_batch_reward': 0.9479166269302368, 'reward': 0.9479166269302368, 'reward_std': 0.15392587520182133, 'kl': 0.2027120441198349, 'clip_ratio': 0.0, 'epoch': 0.6}\n",
      " 90%|███████████████████████████████████    | 180/200 [2:28:23<14:17, 42.85s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 180 that is less than the current step 1621. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 180 that is less than the current step 1621. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_181.txt\n",
      " 90%|███████████████████████████████████▎   | 181/200 [2:30:06<19:19, 61.01s/it]INFO:__main__:Logging to wandb at step 181\n",
      "{'loss': 0.0062, 'grad_norm': 0.66015625, 'learning_rate': 2.2380613335296036e-06, 'completion_length': 650.84375, 'rewards/calculate_batch_reward': 0.9583333134651184, 'reward': 0.9583333134651184, 'reward_std': 0.07013042829930782, 'kl': 0.24131882190704346, 'clip_ratio': 0.0, 'epoch': 0.6}\n",
      " 90%|███████████████████████████████████▎   | 181/200 [2:30:18<19:19, 61.01s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 181 that is less than the current step 1630. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 181 that is less than the current step 1630. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_182.txt\n",
      " 91%|███████████████████████████████████▍   | 182/200 [2:30:52<16:57, 56.50s/it]INFO:__main__:Logging to wandb at step 182\n",
      "{'loss': 0.0381, 'grad_norm': 0.9453125, 'learning_rate': 2.0118056862137357e-06, 'completion_length': 646.375, 'rewards/calculate_batch_reward': 0.8593749701976776, 'reward': 0.8593749701976776, 'reward_std': 0.2910866215825081, 'kl': 0.25948354601860046, 'clip_ratio': 0.0, 'epoch': 0.61}\n",
      " 91%|███████████████████████████████████▍   | 182/200 [2:31:00<16:57, 56.50s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 182 that is less than the current step 1639. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 182 that is less than the current step 1639. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_183.txt\n",
      " 92%|███████████████████████████████████▋   | 183/200 [2:31:28<14:14, 50.24s/it]INFO:__main__:Logging to wandb at step 183\n",
      "{'loss': -0.0015, 'grad_norm': 0.6640625, 'learning_rate': 1.7971252870240291e-06, 'completion_length': 630.21875, 'rewards/calculate_batch_reward': 0.890625, 'reward': 0.890625, 'reward_std': 0.2530868798494339, 'kl': 0.2611551284790039, 'clip_ratio': 0.0, 'epoch': 0.61}\n",
      " 92%|███████████████████████████████████▋   | 183/200 [2:31:28<14:14, 50.24s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 183 that is less than the current step 1648. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 183 that is less than the current step 1648. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_184.txt\n",
      " 92%|███████████████████████████████████▉   | 184/200 [2:31:59<11:53, 44.60s/it]INFO:__main__:Logging to wandb at step 184\n",
      "{'loss': -0.0087, 'grad_norm': 0.78515625, 'learning_rate': 1.59412823400657e-06, 'completion_length': 619.9375, 'rewards/calculate_batch_reward': 0.9374999701976776, 'reward': 0.9374999701976776, 'reward_std': 0.15711415745317936, 'kl': 0.29466182738542557, 'clip_ratio': 0.0, 'epoch': 0.61}\n",
      " 92%|███████████████████████████████████▉   | 184/200 [2:32:00<11:53, 44.60s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 184 that is less than the current step 1657. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 184 that is less than the current step 1657. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_185.txt\n",
      " 92%|████████████████████████████████████   | 185/200 [2:32:52<11:45, 47.06s/it]INFO:__main__:Logging to wandb at step 185\n",
      "{'loss': 0.0529, 'grad_norm': 2.046875, 'learning_rate': 1.4029167422908107e-06, 'completion_length': 655.6875, 'rewards/calculate_batch_reward': 0.9427083134651184, 'reward': 0.9427083134651184, 'reward_std': 0.19110003113746643, 'kl': 0.25515369325876236, 'clip_ratio': 0.0, 'epoch': 0.62}\n",
      " 92%|████████████████████████████████████   | 185/200 [2:32:52<11:45, 47.06s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 185 that is less than the current step 1666. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 185 that is less than the current step 1666. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_186.txt\n",
      " 93%|████████████████████████████████████▎  | 186/200 [2:33:24<09:54, 42.50s/it]INFO:__main__:Logging to wandb at step 186\n",
      "{'loss': -0.0021, 'grad_norm': 0.6953125, 'learning_rate': 1.2235870926211619e-06, 'completion_length': 641.84375, 'rewards/calculate_batch_reward': 0.8854166567325592, 'reward': 0.8854166567325592, 'reward_std': 0.29578250646591187, 'kl': 0.22672541439533234, 'clip_ratio': 0.0, 'epoch': 0.62}\n",
      " 93%|████████████████████████████████████▎  | 186/200 [2:33:35<09:54, 42.50s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 186 that is less than the current step 1675. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 186 that is less than the current step 1675. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_187.txt\n",
      " 94%|████████████████████████████████████▍  | 187/200 [2:34:04<09:03, 41.79s/it]INFO:__main__:Logging to wandb at step 187\n",
      "{'loss': 0.0343, 'grad_norm': 0.81640625, 'learning_rate': 1.0562295828767387e-06, 'completion_length': 634.84375, 'rewards/calculate_batch_reward': 0.9062499701976776, 'reward': 0.9062499701976776, 'reward_std': 0.1680331453680992, 'kl': 0.2043495699763298, 'clip_ratio': 0.0, 'epoch': 0.62}\n",
      " 94%|████████████████████████████████████▍  | 187/200 [2:34:04<09:03, 41.79s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 187 that is less than the current step 1684. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 187 that is less than the current step 1684. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_188.txt\n",
      " 94%|████████████████████████████████████▋  | 188/200 [2:34:35<07:41, 38.50s/it]INFO:__main__:Logging to wandb at step 188\n",
      "{'loss': 0.031, 'grad_norm': 0.828125, 'learning_rate': 9.009284826036691e-07, 'completion_length': 634.96875, 'rewards/calculate_batch_reward': 0.8854166567325592, 'reward': 0.8854166567325592, 'reward_std': 0.29900573194026947, 'kl': 0.23133975267410278, 'clip_ratio': 0.0, 'epoch': 0.63}\n",
      " 94%|████████████████████████████████████▋  | 188/200 [2:35:02<07:41, 38.50s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 188 that is less than the current step 1693. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 188 that is less than the current step 1693. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_189.txt\n",
      " 94%|████████████████████████████████████▊  | 189/200 [2:35:28<07:52, 42.94s/it]INFO:__main__:Logging to wandb at step 189\n",
      "{'loss': 0.0088, 'grad_norm': 0.7421875, 'learning_rate': 7.577619905828282e-07, 'completion_length': 644.03125, 'rewards/calculate_batch_reward': 0.96875, 'reward': 0.96875, 'reward_std': 0.06718549132347107, 'kl': 0.28001952171325684, 'clip_ratio': 0.0, 'epoch': 0.63}\n",
      " 94%|████████████████████████████████████▊  | 189/200 [2:35:38<07:52, 42.94s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 189 that is less than the current step 1702. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 189 that is less than the current step 1702. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_190.txt\n",
      " 95%|█████████████████████████████████████  | 190/200 [2:36:06<06:55, 41.56s/it]INFO:__main__:Logging to wandb at step 190\n",
      "{'loss': 0.0259, 'grad_norm': 0.81640625, 'learning_rate': 6.268021954544096e-07, 'completion_length': 637.125, 'rewards/calculate_batch_reward': 0.8958333432674408, 'reward': 0.8958333432674408, 'reward_std': 0.18094293773174286, 'kl': 0.27953578531742096, 'clip_ratio': 0.0, 'epoch': 0.63}\n",
      " 95%|█████████████████████████████████████  | 190/200 [2:36:06<06:55, 41.56s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 190 that is less than the current step 1711. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 190 that is less than the current step 1711. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_191.txt\n",
      " 96%|█████████████████████████████████████▏ | 191/200 [2:36:41<05:54, 39.43s/it]INFO:__main__:Logging to wandb at step 191\n",
      "{'loss': 0.0271, 'grad_norm': 0.80078125, 'learning_rate': 5.08115039419113e-07, 'completion_length': 659.90625, 'rewards/calculate_batch_reward': 0.859375, 'reward': 0.859375, 'reward_std': 0.24048741906881332, 'kl': 0.3028506934642792, 'clip_ratio': 0.0, 'epoch': 0.64}\n",
      " 96%|█████████████████████████████████████▏ | 191/200 [2:36:41<05:54, 39.43s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 191 that is less than the current step 1720. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 191 that is less than the current step 1720. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_192.txt\n",
      " 96%|█████████████████████████████████████▍ | 192/200 [2:37:14<05:00, 37.61s/it]INFO:__main__:Logging to wandb at step 192\n",
      "{'loss': 0.0111, 'grad_norm': 0.84375, 'learning_rate': 4.0176028503425835e-07, 'completion_length': 643.875, 'rewards/calculate_batch_reward': 0.8229166269302368, 'reward': 0.8229166269302368, 'reward_std': 0.31273049116134644, 'kl': 0.23522952944040298, 'clip_ratio': 0.0, 'epoch': 0.64}\n",
      " 96%|█████████████████████████████████████▍ | 192/200 [2:37:14<05:00, 37.61s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 192 that is less than the current step 1729. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 192 that is less than the current step 1729. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_193.txt\n",
      " 96%|█████████████████████████████████████▋ | 193/200 [2:37:47<04:13, 36.16s/it]INFO:__main__:Logging to wandb at step 193\n",
      "{'loss': 0.0136, 'grad_norm': 0.7578125, 'learning_rate': 3.077914851215585e-07, 'completion_length': 626.5, 'rewards/calculate_batch_reward': 0.8802083134651184, 'reward': 0.8802083134651184, 'reward_std': 0.2273466531187296, 'kl': 0.2196369394659996, 'clip_ratio': 0.0, 'epoch': 0.64}\n",
      " 96%|█████████████████████████████████████▋ | 193/200 [2:37:47<04:13, 36.16s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 193 that is less than the current step 1738. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 193 that is less than the current step 1738. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_194.txt\n",
      " 97%|█████████████████████████████████████▊ | 194/200 [2:38:29<03:46, 37.76s/it]INFO:__main__:Logging to wandb at step 194\n",
      "{'loss': 0.0357, 'grad_norm': 1.078125, 'learning_rate': 2.262559558016325e-07, 'completion_length': 647.40625, 'rewards/calculate_batch_reward': 0.8958333134651184, 'reward': 0.8958333134651184, 'reward_std': 0.22067468240857124, 'kl': 0.3338349014520645, 'clip_ratio': 0.0, 'epoch': 0.65}\n",
      " 97%|█████████████████████████████████████▊ | 194/200 [2:38:36<03:46, 37.76s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 194 that is less than the current step 1747. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 194 that is less than the current step 1747. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_195.txt\n",
      " 98%|██████████████████████████████████████ | 195/200 [2:39:20<03:28, 41.79s/it]INFO:__main__:Logging to wandb at step 195\n",
      "{'loss': 0.0935, 'grad_norm': 1.25, 'learning_rate': 1.571947526689349e-07, 'completion_length': 707.59375, 'rewards/calculate_batch_reward': 0.828125, 'reward': 0.828125, 'reward_std': 0.32383503019809723, 'kl': 0.27475881576538086, 'clip_ratio': 0.0, 'epoch': 0.65}\n",
      " 98%|██████████████████████████████████████ | 195/200 [2:39:20<03:28, 41.79s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 195 that is less than the current step 1756. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 195 that is less than the current step 1756. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "/home/jovyan/.mlspace/envs/deepseek_iana/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py:799: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_196.txt\n",
      " 98%|██████████████████████████████████████▏| 196/200 [2:40:24<03:14, 48.62s/it]INFO:__main__:Logging to wandb at step 196\n",
      "{'loss': 0.0428, 'grad_norm': 0.79296875, 'learning_rate': 1.006426501190233e-07, 'completion_length': 612.375, 'rewards/calculate_batch_reward': 0.9062499701976776, 'reward': 0.9062499701976776, 'reward_std': 0.2604692876338959, 'kl': 0.2596253305673599, 'clip_ratio': 0.0, 'epoch': 0.65}\n",
      " 98%|██████████████████████████████████████▏| 196/200 [2:40:24<03:14, 48.62s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 196 that is less than the current step 1765. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 196 that is less than the current step 1765. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_197.txt\n",
      " 98%|██████████████████████████████████████▍| 197/200 [2:40:58<02:12, 44.04s/it]INFO:__main__:Logging to wandb at step 197\n",
      "{'loss': 0.0017, 'grad_norm': 0.79296875, 'learning_rate': 5.662812383859795e-08, 'completion_length': 637.5625, 'rewards/calculate_batch_reward': 0.9062499701976776, 'reward': 0.9062499701976776, 'reward_std': 0.18135502189397812, 'kl': 0.26220452785491943, 'clip_ratio': 0.0, 'epoch': 0.66}\n",
      " 98%|██████████████████████████████████████▍| 197/200 [2:40:58<02:12, 44.04s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 197 that is less than the current step 1774. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 197 that is less than the current step 1774. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_198.txt\n",
      " 99%|██████████████████████████████████████▌| 198/200 [2:41:27<01:19, 39.71s/it]INFO:__main__:Logging to wandb at step 198\n",
      "{'loss': 0.0118, 'grad_norm': 0.71484375, 'learning_rate': 2.5173336467135267e-08, 'completion_length': 634.34375, 'rewards/calculate_batch_reward': 0.8854166567325592, 'reward': 0.8854166567325592, 'reward_std': 0.25229376554489136, 'kl': 0.23000530898571014, 'clip_ratio': 0.0, 'epoch': 0.66}\n",
      " 99%|██████████████████████████████████████▌| 198/200 [2:41:27<01:19, 39.71s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 198 that is less than the current step 1783. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 198 that is less than the current step 1783. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_199.txt\n",
      "100%|██████████████████████████████████████▊| 199/200 [2:41:57<00:36, 36.77s/it]INFO:__main__:Logging to wandb at step 199\n",
      "{'loss': 0.037, 'grad_norm': 0.734375, 'learning_rate': 6.294126437336734e-09, 'completion_length': 624.28125, 'rewards/calculate_batch_reward': 0.9114583134651184, 'reward': 0.9114583134651184, 'reward_std': 0.25080886483192444, 'kl': 0.2200723960995674, 'clip_ratio': 0.0, 'epoch': 0.66}\n",
      "100%|██████████████████████████████████████▊| 199/200 [2:41:57<00:36, 36.77s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 199 that is less than the current step 1792. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 199 that is less than the current step 1792. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Saved batch outputs to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/outputs-200-DeepSeek-R1-Distill-Qwen-1.5B-GRPO/batch_200.txt\n",
      "100%|███████████████████████████████████████| 200/200 [2:42:44<00:00, 39.66s/it]INFO:__main__:Logging to wandb at step 200\n",
      "{'loss': 0.0, 'grad_norm': 0.79296875, 'learning_rate': 0.0, 'completion_length': 672.3125, 'rewards/calculate_batch_reward': 0.8854166567325592, 'reward': 0.8854166567325592, 'reward_std': 0.2960445284843445, 'kl': 0.2812907248735428, 'clip_ratio': 0.0, 'epoch': 0.67}\n",
      "100%|███████████████████████████████████████| 200/200 [2:42:44<00:00, 39.66s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 1801. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 200 that is less than the current step 1801. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Training stopped.\n",
      "INFO:__main__:Training stopped.\n",
      "INFO:__main__:Training stopped.\n",
      "INFO:__main__:Training stopped.\n",
      "INFO:__main__:Training stopped.\n",
      "INFO:__main__:Training stopped.\n",
      "INFO:__main__:Training stopped.\n",
      "INFO:__main__:Logging to wandb at step 201\n",
      "{'train_runtime': 9808.164, 'train_samples_per_second': 0.653, 'train_steps_per_second': 0.02, 'train_loss': 0.019823137935418345, 'epoch': 0.67}\n",
      "100%|███████████████████████████████████████| 200/200 [2:43:49<00:00, 49.15s/it]\n",
      "INFO:__main__:Training stopped.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading wandb-summary.json 112B/112B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading wandb-summary.json 105B/105B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 619B/619B (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading wandb-summary.json 112B/112B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading wandb-summary.json 105B/105B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading output.log 619B/619B (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading wandb-summary.json 112B/112B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading wandb-summary.json 105B/105B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading output.log 619B/619B (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading wandb-summary.json 112B/112B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading wandb-summary.json 105B/105B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading output.log 619B/619B (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading wandb-summary.json 112B/112B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading wandb-summary.json 105B/105B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 619B/619B (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading wandb-summary.json 112B/112B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading wandb-summary.json 105B/105B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading wandb-summary.json 112B/112B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading wandb-summary.json 105B/105B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading output.log 619B/619B (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading wandb-summary.json 105B/105B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading output.log 619B/619B (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m uploading config.yaml 1.1KB/1.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2D_Layout_20250223_000345\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/6ucsm303\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250223_000345-6ucsm303/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2D_Layout_20250223_000345\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/abda5k44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250223_000345-abda5k44/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2D_Layout_20250223_000345\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/u2gq02r6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250223_000345-u2gq02r6/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2D_Layout_20250223_000345\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/5g6fg6sp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250223_000345-5g6fg6sp/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2D_Layout_20250223_000345\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/vm2otlpu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250223_000345-vm2otlpu/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2D_Layout_20250223_000345\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/63niexqs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250223_000345-63niexqs/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2D_Layout_20250223_000345\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/el55jyxe\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250223_000345-el55jyxe/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 201 that is less than the current step 1802. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "INFO:__main__:Model & tokenizer saved locally.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading output.log 240.6KB/240.6KB (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading wandb-summary.json 807B/807B (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m uploading config.yaml 11.3KB/11.3KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading output.log 240.6KB/240.6KB (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading wandb-summary.json 807B/807B (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m uploading config.yaml 11.3KB/11.3KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading output.log 240.6KB/240.6KB (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading wandb-summary.json 807B/807B (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m uploading config.yaml 11.3KB/11.3KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading output.log 240.6KB/240.6KB (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading wandb-summary.json 807B/807B (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m uploading config.yaml 11.3KB/11.3KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 240.6KB/240.6KB (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading wandb-summary.json 807B/807B (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 11.3KB/11.3KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading output.log 240.6KB/240.6KB (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading wandb-summary.json 807B/807B (0.8s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m uploading config.yaml 11.3KB/11.3KB (0.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: profiling/Time taken: GRPOTrainer._get_per_token_logps ▂▂▇▂▂▂▅▅▂▁▁▂▆▂▆▆▁▁▂▆▁▁▂▅▁▆▂▂▇▆▅▅▅█▇▆▅▆▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      profiling/Time taken: GRPOTrainer._prepare_inputs ▇██▇█▃▆▃▂▃▂▁▂▇▂▅▁▄▂▂▂▂▂▂▃▃▂▂▂▂▂▅▂▂▃▂▁▂▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         profiling/Time taken: GRPOTrainer.compute_loss █▃▃▃▃▃▃▁▁▂▂▂▂▁▁▃▁▂▁▂▁▂▁▁▁▂▁▂▁▁▁▁▂▁▃▁▁▃▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                   step ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                       train/clip_ratio ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                train/completion_length ▆██▆▆▇▇▅▄▅▆▆▃▃▃▄▄▃▃▁▃▃▄▄▅▃▃▃▃▂▃▂▃▃▃▃▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            train/epoch ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                      train/global_step ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                        train/grad_norm ▁▄▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅▆▅▅▅▅▅▄▅█▅▆▅▆▆▆█▆▆▆▆▆▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                               train/kl ▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▄▄▃▄▃▄▄▅▄▄▅▆▅▅▇▆█▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    train/learning_rate ▂▂▂▃▃▅▅▅▇▇██████████▇▇▇▇▆▆▆▆▄▄▄▄▄▃▃▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                             train/loss ▂▂▇▅▆▃▃▇▁▁▄▆▅▂▄▂▃▃▃▃▁▃▆▄▂▄▃▂▄▇▃▆▃▁▄▃▃▄█▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                           train/reward ▁▁▁▁▂▃▄▄▅▆▆▆▆▇▇█▇▇▇▆▆█████████▇▇▇█▇▇█▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                       train/reward_std ▃▃▁▁▇▇▇▇█▆▄▇▅▆▄▅▅▄▆▅▆▃▄▃▄▃▅▄▄█▇▄▄▆▄▅▆▆▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train/rewards/calculate_batch_reward ▁▁▁▂▄▄▄▄▅▅▆▆▆▇▇█▇█▇▇▇▇▇██████▇██▇▇▇▇█▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: profiling/Time taken: GRPOTrainer._get_per_token_logps 0.02607\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      profiling/Time taken: GRPOTrainer._prepare_inputs 18.8279\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         profiling/Time taken: GRPOTrainer.compute_loss 0.03957\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                   step 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                             total_flos 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                       train/clip_ratio 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                train/completion_length 672.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                            train/epoch 0.66667\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                      train/global_step 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                        train/grad_norm 0.79297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                               train/kl 0.28129\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                    train/learning_rate 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                             train/loss 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                           train/reward 0.88542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                       train/reward_std 0.29604\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   train/rewards/calculate_batch_reward 0.88542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                             train_loss 0.01982\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                          train_runtime 9808.164\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               train_samples_per_second 0.653\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                 train_steps_per_second 0.02\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m2D_Layout_20250223_000345\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200/runs/yguzo573\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/iaaaaaaaaaa-nana-aon/Layout-Generation-2d-200\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250223_000345-yguzo573/logs\u001b[0m\n",
      "[rank0]:[W223 02:48:11.950019906 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 accelerate launch \\\n",
    "    --num_processes=8 \\\n",
    "    --num_machines=1 \\\n",
    "    --mixed_precision=bf16 \\\n",
    "    --dynamo_backend=no \\\n",
    "    --main_process_port=29501 \\\n",
    "    /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/train_ac.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d2abe-bcd6-4e5e-8249-b3257dfb1ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc30380-fa85-45a9-b304-75198fd49348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2581b06b-658d-4d13-8a59-b5ba8e8fe23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO\n"
     ]
    }
   ],
   "source": [
    "%cd /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91cf4d8-3c39-4a58-b84c-d820b8b6f5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "538ff653-f735-4b76-9721-9759037dda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "BASE_DIR = \"/home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO\"\n",
    "sys.path.insert(0, os.path.abspath(BASE_DIR))\n",
    "\n",
    "import inference  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e061227f-75ec-410b-b1c0-5c5555bda099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python test_1_5B.py --test_sample_size 100 \\\n",
    "#                     --model_name deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "#                     --checkpoint_path /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/grpo-200_2d_layout-1.5B/checkpoint-180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a70cf953-6be6-404a-b650-d328100918d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/1.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/10.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/100.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/11.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/12.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/13.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/14.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/15.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/16.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/17.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/18.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/19.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/2.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/20.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/21.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/22.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/23.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/24.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/25.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/26.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/27.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/28.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/29.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/3.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/30.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/31.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/32.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/33.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/34.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/35.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/36.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/37.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/38.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/39.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/4.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/40.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/41.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/42.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/43.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/44.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/45.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/46.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/47.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/48.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/49.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/5.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/50.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/51.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/52.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/53.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/54.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/55.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/56.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/57.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/58.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/59.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/6.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/60.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/61.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/62.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/63.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/64.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/65.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/66.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/67.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/68.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/69.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/7.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/70.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/71.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/72.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/73.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/74.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/75.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/76.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/77.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/78.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/79.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/8.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/80.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/81.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/82.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/83.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/84.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/85.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/86.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/87.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/88.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/89.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/9.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/90.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/91.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/92.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/93.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/94.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/95.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/96.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/97.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/98.txt\n",
      "Saved answer: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/99.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/1.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/2.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/3.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/4.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/5.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/6.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/7.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/8.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/9.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/10.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/11.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/12.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/13.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/14.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/15.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/16.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/17.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/18.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/19.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/20.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/21.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/22.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/23.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/24.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/25.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/26.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/27.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/28.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/29.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/30.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/31.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/32.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/33.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/34.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/35.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/36.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/37.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/38.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/39.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/40.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/41.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/42.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/43.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/44.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/45.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/46.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/47.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/48.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/49.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/50.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/51.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/52.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/53.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/54.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/55.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/56.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/57.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/58.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/59.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/60.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/61.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/62.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/63.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/64.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/65.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/66.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/67.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/68.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/69.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/70.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/71.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/72.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/73.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/74.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/75.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/76.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/77.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/78.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/79.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/80.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/81.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/82.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/83.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/84.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/85.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/86.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/87.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/88.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/89.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/90.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/91.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/92.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/93.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/94.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/95.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/96.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/97.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/98.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/99.txt\n",
      "Post-processed answer file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/100.txt\n",
      "Test results saved to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/test_results.txt\n",
      "\n",
      "File    | valid_json | count_reward | bounds_reward | collision_reward | distribution_reward | names_reward | reward\n",
      "--------+------------+--------------+---------------+------------------+---------------------+--------------+-------\n",
      "1.txt   | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "2.txt   | 0.0        | 0.0          | 0.0           | 0.0              | 0.0                 | 0.0          | 0.00  \n",
      "3.txt   | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "4.txt   | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "5.txt   | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "6.txt   | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "7.txt   | 0.0        | 0.0          | 0.0           | 0.0              | 0.0                 | 0.0          | 0.00  \n",
      "8.txt   | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "9.txt   | 1.0        | 1.0          | 1.0           | 0.0              | 1.0                 | 1.0          | 0.83  \n",
      "10.txt  | 1.0        | 1.0          | 0.0           | 1.0              | 1.0                 | 1.0          | 0.83  \n",
      "11.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "12.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "13.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "14.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "15.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "16.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "17.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "18.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "19.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "20.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "21.txt  | 1.0        | 1.0          | 0.0           | 1.0              | 1.0                 | 1.0          | 0.83  \n",
      "22.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "23.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "24.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "25.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "26.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "27.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "28.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "29.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "30.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "31.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "32.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "33.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "34.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "35.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "36.txt  | 1.0        | 1.0          | 1.0           | 0.0              | 1.0                 | 1.0          | 0.83  \n",
      "37.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "38.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "39.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "40.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "41.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "42.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "43.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "44.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "45.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 0.0                 | 1.0          | 0.83  \n",
      "46.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "47.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "48.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "49.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "50.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "51.txt  | 0.0        | 0.0          | 0.0           | 0.0              | 0.0                 | 0.0          | 0.00  \n",
      "52.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "53.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "54.txt  | 1.0        | 1.0          | 1.0           | 0.0              | 1.0                 | 1.0          | 0.83  \n",
      "55.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "56.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "57.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "58.txt  | 1.0        | 1.0          | 0.0           | 1.0              | 1.0                 | 1.0          | 0.83  \n",
      "59.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "60.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "61.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "62.txt  | 1.0        | 1.0          | 1.0           | 0.0              | 1.0                 | 1.0          | 0.83  \n",
      "63.txt  | 1.0        | 1.0          | 0.0           | 1.0              | 1.0                 | 1.0          | 0.83  \n",
      "64.txt  | 1.0        | 1.0          | 1.0           | 0.0              | 0.0                 | 1.0          | 0.67  \n",
      "65.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "66.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "67.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "68.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "69.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "70.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "71.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "72.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "73.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "74.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "75.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "76.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "77.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "78.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "79.txt  | 1.0        | 1.0          | 0.0           | 1.0              | 1.0                 | 1.0          | 0.83  \n",
      "80.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "81.txt  | 1.0        | 1.0          | 1.0           | 0.0              | 1.0                 | 1.0          | 0.83  \n",
      "82.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "83.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "84.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "85.txt  | 0.0        | 0.0          | 0.0           | 0.0              | 0.0                 | 0.0          | 0.00  \n",
      "86.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "87.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "88.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "89.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "90.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "91.txt  | 0.0        | 0.0          | 0.0           | 0.0              | 0.0                 | 0.0          | 0.00  \n",
      "92.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "93.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "94.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "95.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "96.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "97.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "98.txt  | 1.0        | 1.0          | 0.0           | 1.0              | 1.0                 | 1.0          | 0.83  \n",
      "99.txt  | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "100.txt | 1.0        | 1.0          | 1.0           | 1.0              | 1.0                 | 1.0          | 1.00  \n",
      "\n",
      "Total files: 100\n",
      "Valid JSON: 95.0/100\n",
      "Count reward: 95.0/100\n",
      "Bounds reward: 89.0/100\n",
      "Collision reward: 89.0/100\n",
      "Distribution reward: 93.0/100\n",
      "Names reward: 95.0/100\n",
      "Average overall reward: 0.93\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python test_1_5B.py --prompts_dir /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/prompts \\\n",
    "                    --answers_dir /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2 \\\n",
    "                    --model_name deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "                    --checkpoint_path /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/grpo-200_2d_layout-1.5B/checkpoint-180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6c31a-52e7-4a32-b5f4-15a2766ca64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "669f33e2-6be5-4895-ab81-7c8ff298049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/1.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/1.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/10.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/10.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/100.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/100.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/11.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/11.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/12.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/12.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/13.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/13.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/14.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/14.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/15.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/15.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/16.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/16.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/17.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/17.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/18.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/18.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/19.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/19.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/2.txt\n",
      "Failed to load /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/2.txt: Expecting value: line 1 column 1 (char 0)\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/20.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/20.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/21.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/21.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/22.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/22.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/23.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/23.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/24.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/24.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/25.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/25.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/26.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/26.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/27.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/27.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/28.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/28.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/29.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/29.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/3.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/3.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/30.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/30.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/31.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/31.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/32.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/32.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/33.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/33.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/34.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/34.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/35.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/35.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/36.txt\n",
      "Overlapping objects detected:\n",
      " - Sofa overlaps with Chair\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/36.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/37.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/37.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/38.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/38.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/39.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/39.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/4.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/4.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/40.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/40.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/41.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/41.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/42.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/42.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/43.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/43.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/44.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/44.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/45.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/45.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/46.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/46.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/47.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/47.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/48.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/48.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/49.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/49.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/5.txt\n",
      "Overlapping objects detected:\n",
      " - Sofa overlaps with Table\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/5.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/50.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/50.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/51.txt\n",
      "Failed to load /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/51.txt: Expecting value: line 1 column 1 (char 0)\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/52.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/52.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/53.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/53.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/54.txt\n",
      "Overlapping objects detected:\n",
      " - Sofa overlaps with Table\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/54.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/55.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/55.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/56.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/56.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/57.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/57.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/58.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/58.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/59.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/59.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/6.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/6.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/60.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/60.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/61.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/61.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/62.txt\n",
      "Overlapping objects detected:\n",
      " - Coffee Table overlaps with Bookshelf\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/62.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/63.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/63.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/64.txt\n",
      "Overlapping objects detected:\n",
      " - Bed overlaps with Wardrobe\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/64.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/65.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/65.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/66.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/66.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/67.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/67.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/68.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/68.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/69.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/69.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/7.txt\n",
      "Failed to load /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/7.txt: Expecting value: line 1 column 1 (char 0)\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/70.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/70.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/71.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/71.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/72.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/72.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/73.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/73.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/74.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/74.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/75.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/75.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/76.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/76.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/77.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/77.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/78.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/78.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/79.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/79.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/8.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/8.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/80.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/80.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/81.txt\n",
      "Overlapping objects detected:\n",
      " - Sofa overlaps with Wardrobe\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/81.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/82.txt\n",
      "Overlapping objects detected:\n",
      " - Chair overlaps with Wardrobe\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/82.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/83.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/83.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/84.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/84.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/85.txt\n",
      "Failed to load /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/85.txt: Expecting value: line 1 column 1 (char 0)\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/86.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/86.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/87.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/87.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/88.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/88.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/89.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/89.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/9.txt\n",
      "Overlapping objects detected:\n",
      " - Sofa overlaps with Wardrobe\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/9.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/90.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/90.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/91.txt\n",
      "Failed to load /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/91.txt: Expecting value: line 1 column 1 (char 0)\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/92.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/92.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/93.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/93.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/94.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/94.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/95.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/95.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/96.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/96.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/97.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/97.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/98.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/98.png\n",
      "Processing file: /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2/99.txt\n",
      "No overlapping objects detected.\n",
      "Saved plot to /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2/99.png\n"
     ]
    }
   ],
   "source": [
    "!python visualize_answers.py --prompts_dir /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/prompts \\\n",
    "                            --answers_dir /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/answers_2 \\\n",
    "                            --output_dir /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/images_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd544548-e0df-4cf8-8240-a225ffda2eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c92d67-c39d-419b-a13f-35ee4198f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python inference.py --prompts_dir /path/to/prompts \\\n",
    "#                     --answers_dir /path/to/answers \\\n",
    "#                     --model_name deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "#                     --checkpoint_path /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/grpo2_2d_layout-1.5B/checkpoint-140\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0916a791-32d1-4977-9958-b8ea1dc341f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python test_1.5B.py --prompts_dir /path/to/prompts \\\n",
    "#                     --answers_dir /path/to/save/answers \\\n",
    "#                     --model_name deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
    "#                     --checkpoint_path /home/jovyan/shares/SR008.fs2/iana_kulichenko/Experiments/train_GRPO/grpo2_2d_layout-1.5B/checkpoint-140\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52bed2-e129-4403-b5c0-7769748d2266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24eabd-9608-4fc0-9099-35f03d863f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-deepseek_iana]",
   "language": "python",
   "name": "conda-env-.mlspace-deepseek_iana-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
